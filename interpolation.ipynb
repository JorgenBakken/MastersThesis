{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import lagrange\n",
    "from numpy.polynomial.chebyshev import chebgauss\n",
    "from scipy.interpolate import BarycentricInterpolator\n",
    "import math\n",
    "\n",
    "def interval_mapping(x, a, b): \n",
    "    \"\"\"\n",
    "    Maps x on the interval [-1, 1] to the interval [a, b]\n",
    "    \"\"\"\n",
    "    return 0.5 * (b - a) * x + 0.5 * (a + b)\n",
    "\n",
    "# Define the Runge function\n",
    "def f(x):\n",
    "    return 1 / (1 + x**2)\n",
    "\n",
    "# Generate 10 equidistant nodes and 10 Chebyshev nodes on the interval [-5, 5]\n",
    "def generate_nodes(n, a, b):\n",
    "    x_eq = np.linspace(a, b, n) \n",
    "    x, _ = chebgauss(n)\n",
    "    x_ch = interval_mapping(x, a, b)\n",
    "    return x_eq, x_ch\n",
    "\n",
    "n_max = 10\n",
    "a, b = -5, 5\n",
    "N = n_max * 100\n",
    "\n",
    "# Generate equidistant and Chebyshev nodes\n",
    "x_eq, x_ch = generate_nodes(n_max, a, b)\n",
    "x_eval = np.linspace(a, b, N)\n",
    "\n",
    "y_eq = lagrange(x_eq, f(x_eq))(x_eval)\n",
    "y_ch = lagrange(x_ch, f(x_ch))(x_eval)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot for equidistant nodes\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_eval, f(x_eval), label='Runge function')\n",
    "plt.plot(x_eval, y_eq, label='Lagrange interpolation (equidistant)', linestyle='--')\n",
    "plt.scatter(x_eq, f(x_eq), color='red', label='Equidistant nodes')\n",
    "plt.title('Lagrange interpolation with Equidistant Nodes')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Chebyshev nodes\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_eval, f(x_eval), label='Runge function')\n",
    "plt.plot(x_eval, y_ch, label='Lagrange interpolation (Chebyshev)', linestyle='--')\n",
    "plt.scatter(x_ch, f(x_ch), color='red', label='Chebyshev nodes')\n",
    "plt.title('Lagrange interpolation with Chebyshev Nodes')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def tictoc(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        tic = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        toc = time.time()\n",
    "        print(f\"Elapsed time: {toc - tic} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the given functions\n",
    "def f1(x):\n",
    "    return np.cos(2 * np.pi * x)\n",
    "\n",
    "def f2(x):\n",
    "    return np.exp(3 * x) * np.sin(2 * x)\n",
    "\n",
    "def max_norm(y_true, y_pred):\n",
    "    return np.max(np.abs(y_true - y_pred))\n",
    "\n",
    "def l2_norm(y_true, y_pred, a, b, N):\n",
    "    return np.sqrt((b-a)/N * np.sum((y_true - y_pred)**2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 1\n",
    "err_eq_max = []\n",
    "err_ch_max = []\n",
    "err_eq_l2 = []\n",
    "err_ch_l2 = []\n",
    "\n",
    "n_lst = [2, 4, 8, 16, 32]\n",
    "\n",
    "for n_max in n_lst:\n",
    "    N = n_max * 100\n",
    "    \n",
    "    x_eq, x_ch = generate_nodes(n_max, a, b)\n",
    "\n",
    "    x_eval = np.linspace(a, b, N)\n",
    "    y1_eq = lagrange(x_eq, f1(x_eq))(x_eval)\n",
    "    y1_ch = lagrange(x_ch, f1(x_ch))(x_eval)    \n",
    "    # y1_eq = BarycentricInterpolator(x_eq, f1(x_eq))(x_eval)\n",
    "    # y1_ch = BarycentricInterpolator(x_ch, f1(x_ch))(x_eval)\n",
    "\n",
    "    err_eq_max.append(max_norm(f1(x_eval), y1_eq))\n",
    "    err_ch_max.append(max_norm(f1(x_eval), y1_ch))\n",
    "    err_eq_l2.append(l2_norm(f1(x_eval), y1_eq, a, b, N))\n",
    "    err_ch_l2.append(l2_norm(f1(x_eval), y1_ch, a, b, N))\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(n_lst, err_eq_max, label='Equidistant nodes')\n",
    "plt.plot(n_lst, err_ch_max, label='Chebyshev nodes')\n",
    "plt.title('Max Norm Error')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Error')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(n_lst, err_eq_l2, label='Equidistant nodes')\n",
    "plt.plot(n_lst, err_ch_l2, label='Chebyshev nodes')\n",
    "plt.title('L2 Norm Error')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Error')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = np.pi/4\n",
    "\n",
    "err_eq_max = []\n",
    "err_ch_max = []\n",
    "err_eq_l2 = []\n",
    "err_ch_l2 = []\n",
    "\n",
    "for n_max in n_lst:\n",
    "    N = n_max * 100\n",
    "\n",
    "    x_eq, x_ch = generate_nodes(n_max, a, b)\n",
    "\n",
    "    x_eval = np.linspace(a, b, N)\n",
    "    y2_eq = lagrange(x_eq, f2(x_eq))(x_eval)\n",
    "    y2_ch = lagrange(x_ch, f2(x_ch))(x_eval)\n",
    "    # y2_eq = BarycentricInterpolator(x_eq, f2(x_eq))(x_eval)\n",
    "    # y2_ch = BarycentricInterpolator(x_ch, f2(x_ch))(x_eval)\n",
    "\n",
    "    err_eq_max.append(max_norm(f2(x_eval), y2_eq))\n",
    "    err_ch_max.append(max_norm(f2(x_eval), y2_ch))\n",
    "    err_eq_l2.append(l2_norm(f2(x_eval), y2_eq, a, b, N))\n",
    "    err_ch_l2.append(l2_norm(f2(x_eval), y2_ch, a, b, N))\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(n_lst, err_eq_max, label='Equidistant nodes')\n",
    "plt.plot(n_lst, err_ch_max, label='Chebyshev nodes')\n",
    "plt.title('Max Norm Error')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Error')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(n_lst, err_eq_l2, label='Equidistant nodes')\n",
    "plt.plot(n_lst, err_ch_l2, label='Chebyshev nodes')\n",
    "plt.title('L2 Norm Error')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('Error')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebyshev(n, a, b):\n",
    "    return (a + b) / 2 + (b - a) / 2 * np.cos((2 * np.arange(n) + 1) / (2 * n) * np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runge(x):\n",
    "    return (1/(x**2+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagInterp(x, y, evaluate):\n",
    "    \"\"\"\n",
    "    Performs lagrangian interpolation.\n",
    "    \n",
    "    Inputs: \n",
    "    x = an array of nodes\n",
    "    y = the function evaluated at these nodes\n",
    "    evaluate = points at which we evaluate the interpolation\n",
    "    \n",
    "    Outputs:\n",
    "    solution = interpolation polynomial evaluated at the values in evaluate\n",
    "    \"\"\"\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Arrays x and y must have the same length\")\n",
    "    if len(np.unique(x)) != len(x):\n",
    "        raise ValueError(\"Nodes in x must be unique\")\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    evaluate = np.array(evaluate)\n",
    "    solution = np.zeros_like(evaluate, dtype=float)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        li = np.ones_like(evaluate)\n",
    "        for j in range(len(x)):\n",
    "            if j != i:\n",
    "                li *= (evaluate - x[j]) / (x[i] - x[j])\n",
    "        solution += li * y[i]\n",
    "    \n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing for the Runge function on the interval [-5,5] for n=10\n",
    "\n",
    "#Number of nodes\n",
    "n = 10\n",
    "\n",
    "#Generating Chebyshev nodes and points at which the function is known\n",
    "x_cheb = chebyshev(n, -5, 5)\n",
    "y_cheb = runge(x_cheb)\n",
    "\n",
    "#Evaluation points\n",
    "eval_cheb = np.linspace(-5,5,100)\n",
    "\n",
    "#Interpolation on Chebyshev nodes\n",
    "# cheb = lagInterp(x_cheb, y_cheb, eval_cheb)\n",
    "cheb = lagrange(x_cheb, y_cheb)(eval_cheb)\n",
    "\n",
    "#Generating equidistant nodes and points at which the function is known\n",
    "x_equi = np.linspace(-5,5, n)\n",
    "y_equi = runge(x_equi)\n",
    "\n",
    "#Evaluation points\n",
    "eval_equi = np.linspace(-5,5,100)\n",
    "\n",
    "#Interpolation on Chebyshev nodes\n",
    "# equi = lagInterp(x_equi, y_equi, eval_equi)\n",
    "equi = lagrange(x_equi, y_equi)(eval_equi)\n",
    "\n",
    "#True function\n",
    "x_true = np.linspace(-5,5,1000)\n",
    "y_true = runge(x_true)\n",
    "\n",
    "plt.plot(eval_equi, equi, 'g', label = \"Equidistant nodes\")\n",
    "plt.plot(x_true, y_true, 'r', label = \"True function\")\n",
    "plt.plot(eval_cheb, cheb, 'b', label = \"Chebyshev nodes\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "#plt.savefig(path+\"runge_a_2\", bbox_extra_artists=[legend,], bbox_inches='tight')\n",
    "#Chebyshev nodes converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we increase n:\n",
    "#Implementing for the Runge function on the interval [-5,5] for n=20\n",
    "n = 20\n",
    "x_cheb = chebyshev(n, -5, 5)\n",
    "y_cheb = runge(x_cheb)\n",
    "eval_cheb = np.linspace(-5,5,100)\n",
    "cheb = lagInterp(x_cheb, y_cheb, eval_cheb)\n",
    "\n",
    "x_equi = np.linspace(-5,5, n)\n",
    "y_equi = runge(x_equi)\n",
    "eval_equi = np.linspace(-5,5,100)\n",
    "equi = lagInterp(x_equi, y_equi, eval_equi)\n",
    "\n",
    "x_true = np.linspace(-5,5,1000)\n",
    "y_true = runge(x_true)\n",
    "plt.plot(x_true, y_true, 'r', label = \"True function\")\n",
    "plt.plot(eval_cheb, cheb, 'b', label = \"Chebyshev nodes\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At a major increase in n, n=50:\n",
    "n = 50\n",
    "x_cheb = chebyshev(n, -5, 5)\n",
    "y_cheb = runge(x_cheb)\n",
    "eval_cheb = np.linspace(-5,5,100)\n",
    "cheb = lagInterp(x_cheb, y_cheb, eval_cheb)\n",
    "\n",
    "x_equi = np.linspace(-5,5, n)\n",
    "y_equi = runge(x_equi)\n",
    "eval_equi = np.linspace(-5,5,100)\n",
    "equi = lagInterp(x_equi, y_equi, eval_equi)\n",
    "\n",
    "x_true = np.linspace(-5,5,1000)\n",
    "y_true = runge(x_true)\n",
    "plt.plot(x_true, y_true, 'r', label = \"True function\")\n",
    "plt.plot(eval_cheb, cheb, 'b', label = \"Chebyshev nodes\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_1(x):\n",
    "    return np.cos(2*np.pi*x)\n",
    "    \n",
    "def f_2(x):\n",
    "    return (np.exp(3*x))*np.sin(2*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infNorm(analytical, numerical):\n",
    "    return np.linalg.norm(analytical - numerical, np.inf)\n",
    "\n",
    "def twoNorm(analytical, numerical, interval):\n",
    "    return np.sqrt(interval[1]-interval[0])/np.sqrt(len(analytical)) * np.sqrt(np.sum((analytical-numerical)**2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nVals = np.arange(1, 25, 2)\n",
    "\n",
    "iNorm = []\n",
    "tNorm = []\n",
    "for n in nVals:\n",
    "    #print(\"n = \", n)\n",
    "    nLarge = 100*n\n",
    "    \n",
    "    #Generate nodes and apply the function to them\n",
    "    x = np.linspace(0, 1, n)\n",
    "    y = f_1(x)\n",
    "    \n",
    "    #Generate evaluation points\n",
    "    manyPoints = np.linspace(0, 1,nLarge)\n",
    "    \n",
    "    #Interpolate\n",
    "    interpolation = lagInterp(x, y, manyPoints)\n",
    "    # interpolation = lagrange(x, y)(manyPoints)\n",
    "    \n",
    "    #Calculate norms\n",
    "    iNorm += [infNorm(f_1(manyPoints), interpolation)]\n",
    "    tNorm += [twoNorm(f_1(manyPoints), interpolation, [0,1])]\n",
    "    \n",
    "\n",
    "plt.semilogy(nVals, iNorm, 'r', label=\"Max norm\")\n",
    "plt.semilogy(nVals, tNorm, 'b', label=\"Two norm\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Error estimate\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making estimated error plots for chebyshev nodes for f_1\n",
    "nVals = np.arange(1, 50, 2)\n",
    "\n",
    "iNorm = []\n",
    "tNorm = []\n",
    "for n in nVals:\n",
    "    #print(\"n = \", n)\n",
    "    nLarge = 100*n\n",
    "    \n",
    "    #Generate nodes and apply the function to them\n",
    "    x = chebyshev(n, 0, 1)\n",
    "    y = f_1(x)\n",
    "    \n",
    "    #Generate evaluation points\n",
    "    manyPoints = np.linspace(0, 1, nLarge)\n",
    "    \n",
    "    #Interpolate\n",
    "    interpolation = lagInterp(x, y, manyPoints)\n",
    "    \n",
    "    #Calculate norms\n",
    "    iNorm += [infNorm(f_1(manyPoints), interpolation)]\n",
    "    tNorm += [twoNorm(f_1(manyPoints), interpolation, [0,1])]\n",
    "    \n",
    "\n",
    "plt.semilogy(nVals, iNorm, 'r', label=\"Max norm\")\n",
    "plt.semilogy(nVals, tNorm, 'b', label=\"Two norm\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Error estimate\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For function f_2 equidistant nodes\n",
    "nVals = np.arange(1, 25, 1)\n",
    "iNorm = np.array([])\n",
    "tNorm = np.array([])\n",
    "\n",
    "for n in nVals:\n",
    "    #print(\"This is n\", n)\n",
    "    nLarge = 100*n\n",
    "    \n",
    "    #Generate nodes and apply the function to them\n",
    "    x = np.linspace(0, np.pi/4, n)\n",
    "    y = f_2(x)\n",
    "    \n",
    "    #Generate the evaluation points\n",
    "    manyPoints = np.linspace(0, np.pi/4,nLarge)\n",
    "    \n",
    "    #Interpolate\n",
    "    interpolation = lagInterp(x, y, manyPoints)\n",
    "    \n",
    "    #Calculate norms\n",
    "    tNorm = np.append(tNorm,twoNorm(f_2(manyPoints), interpolation, [0,1]))\n",
    "    iNorm = np.append(iNorm,infNorm(f_2(manyPoints), interpolation))\n",
    "                              \n",
    "\n",
    "\n",
    "plt.semilogy(nVals, iNorm, 'r', label=\"Max norm\")\n",
    "plt.semilogy(nVals, tNorm, 'b', label=\"Two norm\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Error estimate\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For function f_2 chebyshev nodes\n",
    "nVals = np.arange(1, 50, 2)\n",
    "iNorm = np.array([])\n",
    "tNorm = np.array([])\n",
    "for n in nVals:\n",
    "    #print(\"We are in n\", n)\n",
    "    nLarge = 100*n\n",
    "    \n",
    "    #Generate nodes and apply the function to them\n",
    "    x = chebyshev(n, 0, np.pi/4)\n",
    "    y = f_2(x)\n",
    "    \n",
    "    #Generate evaluation points\n",
    "    manyPoints = np.linspace(0, np.pi/4,nLarge)\n",
    "    \n",
    "    #Interpolate\n",
    "    interpolation = lagInterp(x, y, manyPoints)\n",
    "    \n",
    "    #Calculate norms\n",
    "    tNorm = np.append(tNorm,twoNorm(f_2(manyPoints), interpolation, [0,1]))\n",
    "    iNorm = np.append(iNorm,infNorm(f_2(manyPoints), interpolation))\n",
    "                              \n",
    "\n",
    "\n",
    "plt.semilogy(nVals, iNorm, 'r', label=\"Max norm\")\n",
    "plt.semilogy(nVals, tNorm, 'b', label=\"Two norm\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Error estimate\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making plot for both theoretical error bound and error estimates for equidistant nodes and f_1\n",
    "nVals = np.arange(1, 25, 1)\n",
    "iNorm = []\n",
    "tNorm = []\n",
    "error_list = []\n",
    "for n in nVals:\n",
    "    #print(\"n = \", n)\n",
    "    nLarge = 100*n\n",
    "    \n",
    "    #Generate nodes and apply the function to them\n",
    "    x = np.linspace(0, 1, n)\n",
    "    y = f_1(x)\n",
    "    \n",
    "    #Generate evaluation points\n",
    "    manyPoints = np.linspace(0, 1,nLarge)\n",
    "    \n",
    "    #Interpolate\n",
    "    interpolation = lagInterp(x, y, manyPoints)\n",
    "    \n",
    "    #Calculate norms\n",
    "    iNorm += [infNorm(f_1(manyPoints), interpolation)]\n",
    "    tNorm += [twoNorm(f_1(manyPoints), interpolation, [0,1])]\n",
    "    \n",
    "    #Calculating the error bound\n",
    "    big_N = np.linspace(0, 1, 100*n)\n",
    "    x=np.linspace(0,1,n, dtype=float)\n",
    "    \n",
    "    func_derivative = (2*np.pi)**(n+1)\n",
    "    #print(\"Func_derivativ\", func_derivative)\n",
    "    \n",
    "    intermediate = 0\n",
    "    for item in big_N:\n",
    "        val = 1\n",
    "        for node in x:\n",
    "            if(item==node):\n",
    "                continue\n",
    "            val *= (item - node)\n",
    "        #print(\"Val\", val)\n",
    "        intermediate += np.abs(val)\n",
    "    \n",
    "    \n",
    "    denom = math.factorial(n+1)\n",
    "    final = (func_derivative*intermediate)/denom\n",
    "    error_list += [final]\n",
    "    \n",
    "\n",
    "plt.semilogy(nVals, iNorm, 'r', label=\"Max norm\")\n",
    "plt.semilogy(nVals, tNorm, 'b', label=\"Two norm\")\n",
    "plt.semilogy(nVals, error_list, 'g', label = \"Error bound\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Error estimate\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "#plt.savefig(path+\"error_f1_equi_with_error_bound\", bbox_extra_artists=[legend,], bbox_inches='tight')\n",
    "\n",
    "#plt.title(\"function and derivative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pieceWise(n_sub_int, interval, function, nodes=5):\n",
    "    \"\"\"\n",
    "    An implementation of piecewise lagrangian interpolation\n",
    "    \n",
    "    Input:\n",
    "    n_sub_int = number of subintervals in which to divide the interval\n",
    "    interval = interval of the function\n",
    "    function = function under consideration\n",
    "    \n",
    "    Output:\n",
    "    solution = an array of interpolated values\n",
    "    functionPoints = an array of the nodes in each subinterval\n",
    "    infNorm(...) = the max norm of the method\n",
    "    twoNorm(...) = the two norm of the method\n",
    "    \"\"\"\n",
    "    \n",
    "    solution = np.array([])\n",
    "    functionPoints = np.array([])\n",
    "    subIntervals = np.linspace(interval[0], interval[1], n_sub_int)\n",
    "    for i in range(1, len(subIntervals)):\n",
    "        \n",
    "        #Generate nodes and apply the function to them\n",
    "        x = np.linspace(subIntervals[i-1],subIntervals[i],nodes+1)\n",
    "        y = function(x)\n",
    "        \n",
    "        #Interpolate\n",
    "        interpolation = lagInterp(x, y, np.linspace(subIntervals[i-1],subIntervals[i],nodes*10))\n",
    "        subIntPoints = np.linspace(subIntervals[i-1],subIntervals[i],nodes*10)\n",
    "        \n",
    "        #Append solution and nodes used\n",
    "        solution = np.append(solution, interpolation)\n",
    "        functionPoints = np.append(functionPoints, subIntPoints)\n",
    "    return solution, functionPoints, infNorm(function(functionPoints), solution), twoNorm(function(functionPoints), solution, interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infList = np.array([])\n",
    "twoList = np.array([])\n",
    "n_vals = np.arange(1, 11, 1)\n",
    "\n",
    "for n in n_vals:\n",
    "    _, _, infl, twol = pieceWise(8, [-1,1], runge, n)\n",
    "    infList = np.append(infList, infl)\n",
    "    twoList = np.append(twoList, twol)\n",
    "plt.semilogy(n_vals, infList, 'r',  label = \"Max norm\")\n",
    "plt.semilogy( n_vals, twoList, 'b', label = \"Two norm\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Error estimate\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kVals = np.arange(2, 21)\n",
    "\n",
    "for n in np.arange(1,11,1):\n",
    "    normList = np.array([])\n",
    "    for k in kVals:\n",
    "        _, _, normval, _ = pieceWise(k, [-1,1], runge, n)\n",
    "        normList = np.append(normList, normval)\n",
    "        #print(\"We are on k=\", k)\n",
    "    plt.semilogy(normList, label=\"n =\"+str(n))\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"Max norm\")\n",
    "    legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normList = np.array([])\n",
    "kVals = np.arange(2, 21)\n",
    "for k in kVals:\n",
    "    _, _, normval, _ = pieceWise(k, [-1,1], runge, 10)\n",
    "    normList = np.append(normList, normval)\n",
    "\n",
    "plt.semilogy(kVals, normList)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Max norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normListStandard = np.array([])\n",
    "normListPiecewise = np.array([])\n",
    "normListCheb = np.array([])\n",
    "nVals = np.arange(1,16, 1)\n",
    "for n in nVals:\n",
    "    #Piecewise method\n",
    "    _, _, normval, _ = pieceWise(k, [-1,1], runge, n)\n",
    "    normListPiecewise = np.append(normListPiecewise, normval)\n",
    "    \n",
    "    nLarge = 100*n\n",
    "    \n",
    "    #Generate nodes and apply the function to them\n",
    "    x = np.linspace(-1, 1, n)\n",
    "    y = runge(x)\n",
    "    z1 = chebyshev(n, -1, 1)\n",
    "    z2 = runge(z1)\n",
    "    \n",
    "    #Generate evaluation points\n",
    "    manyPoints = np.linspace(-1, 1,nLarge)\n",
    "    \n",
    "    #Interpolate\n",
    "    interpolation = lagInterp(x, y, manyPoints)\n",
    "    cheb = lagInterp(z1, z2, manyPoints)\n",
    "    \n",
    "    normListStandard = np.append(normListStandard, infNorm(runge(manyPoints), interpolation))\n",
    "    normListCheb = np.append(normListCheb,  infNorm(runge(manyPoints), cheb))\n",
    "\n",
    "plt.semilogy(nVals, normListStandard, 'r', label = \"Std. equi\")\n",
    "plt.semilogy(nVals, normListCheb, 'g', label = \"Std. cheby\")\n",
    "plt.semilogy(nVals, normListPiecewise, 'b', label = \"Piecewise method\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Max norm\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_piecwise = np.array([])\n",
    "time_lagrangian = np.array([])\n",
    "time_cheby = np.array([])\n",
    "nVals = np.arange(1, 16, 1)\n",
    "for n in nVals:\n",
    "    #Piecewise method\n",
    "    piece = time.time()\n",
    "    _, _, _, _ = pieceWise(k, [-1,1], runge, n)\n",
    "    time_piecwise = np.append(time_piecwise, time.time() - piece)\n",
    "    \n",
    "    nLarge = 100*n\n",
    "    \n",
    "    #Generate nodes and apply the function to them\n",
    "    x = np.linspace(-1, 1, n)\n",
    "    y = runge(x)\n",
    "    z1 = chebyshev(n, -1, 1)\n",
    "    z2 = runge(z1)\n",
    "    \n",
    "    #Generate evaluation points\n",
    "    manyPoints = np.linspace(-1, 1,nLarge)\n",
    "    \n",
    "    #Interpolate\n",
    "    lagrangian = time.time()\n",
    "    interpolation = lagInterp(x, y, manyPoints)\n",
    "    time_lagrangian = np.append(time_lagrangian, time.time() - lagrangian)\n",
    "    \n",
    "    cheb_time = time.time()\n",
    "    cheby = lagInterp(z1, z2, manyPoints)\n",
    "    time_cheby = np.append(time_cheby, time.time()- cheb_time)\n",
    "    \n",
    "\n",
    "plt.semilogy(nVals, time_lagrangian, 'r', label = \"Standard method\")\n",
    "plt.semilogy(nVals, time_piecwise, 'b', label = \"Piecewise method\")\n",
    "plt.semilogy(nVals, time_cheby, 'g', label = \"Std. cheby\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Time to run\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions are used for both implementations of gradient descent.\n",
    "\n",
    "def interp(x, x_eval, f):\n",
    "    \"\"\"\n",
    "    A vectorized implementation of the Lagrange interpolation.\n",
    "    \n",
    "    Inputs:\n",
    "    x = array of nodes\n",
    "    x_eval = array or scalar of values at which to evaluate the interpolation\n",
    "    f = function we wish to interpolate\n",
    "    \n",
    "    Output:\n",
    "    poly = interpolated polynomial evaluated in x_eval\n",
    "    \"\"\"\n",
    "    if len(x) != len(set(x)):\n",
    "        raise ValueError(\"Nodes must be unique\")\n",
    "    \n",
    "    n = len(x)\n",
    "    L = np.zeros_like(x_eval, dtype=float)\n",
    "    \n",
    "    # Compute the interpolation for each evaluation point\n",
    "    for i in range(n):\n",
    "        terms = np.ones_like(x_eval)\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                terms *= (x_eval - x[j]) / (x[i] - x[j])\n",
    "        L += terms * f(x[i])\n",
    "    \n",
    "    return L\n",
    "\n",
    "def create_cost_function(eta, interval, f):\n",
    "    def C(x):\n",
    "        val = np.sum((f(eta) - interp(x, eta, f))**2)\n",
    "        return ((interval[1] - interval[0]) / len(eta)) * val\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_gd(x, eta, l, down, up, interval, function, max_iter = 100, tol = 1e-10):\n",
    "    \"\"\"\n",
    "    Gradient descent with backtracking implemented along the \n",
    "    lines of the pseudocode in the project description\n",
    "    \n",
    "    Input:\n",
    "    x = an array of interpolation nodes\n",
    "    eta = the x values for which we know function(x)\n",
    "    l = step length hyperparameter\n",
    "    down = hyperparameter for reducing step length\n",
    "    up = hyperparameter for increasing step length\n",
    "    interval = interval of the function\n",
    "    function = function under consideration\n",
    "    max_iter = maximum number of iterations\n",
    "    tol = tolerance after which we terminate the algorithm\n",
    "    \n",
    "    Output:\n",
    "    an array of interpolation nodes\n",
    "    \"\"\"\n",
    "    cost = create_cost_function(eta, interval, function) \n",
    "    \n",
    "    x = x\n",
    "    l = l\n",
    "    interval = interval\n",
    "    gradient = grad(cost,0)\n",
    "    p_k = gradient(x)\n",
    "    norm_p_k = np.linalg.norm(p_k)\n",
    "    grad_norm_list = np.array([norm_p_k])\n",
    "    costs = cost(x)\n",
    "    iterations_at_minima = 0\n",
    "    \n",
    "    while max_iter>0 and norm_p_k>tol:\n",
    "        print(\"We are on iteration\", max_iter)\n",
    "        #print(\"This is p_k\", p_k)\n",
    "        #print(\"This is the norm\", norm_p_k)\n",
    "        #print(\"This is x\", x)\n",
    "        \n",
    "        #Generating new step\n",
    "        new_x = x - (1/l)*p_k\n",
    "        \n",
    "        #Testing if step satisfies decrease condition\n",
    "        while cost(new_x) > cost(x) + np.dot(p_k,(-(1/l)*p_k)) + (l/2)*(np.linalg.norm(-(1/l)*p_k)**2):\n",
    "            #If fail, decrease step length\n",
    "            l = down*l\n",
    "            new_x = x - (1/l)*p_k\n",
    "            \n",
    "        #If success, take step and calculate new gradient.\n",
    "        x = new_x\n",
    "        p_k = gradient(x)\n",
    "        norm_p_k = np.linalg.norm(p_k)\n",
    "        \n",
    "        #Increase step length\n",
    "        l = up*l\n",
    "        max_iter -= 1\n",
    "        \n",
    "        costs = np.append(costs, cost(x))\n",
    "        grad_norm_list = np.append(grad_norm_list, np.linalg.norm(p_k))\n",
    "        \n",
    "        if (np.abs(grad_norm_list[-1]-grad_norm_list[-2])<1e-15):\n",
    "            return x, grad_norm_list, costs\n",
    "        \n",
    "    return x, grad_norm_list, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x, eta, interval, function, max_iter = 40, tol = 1e-7\n",
    "def pieceWise_gd(numberOfSubintervals, l, down, up, interval, function, nodes=4):\n",
    "    \"\"\"\n",
    "    An adaptation of the piecewise method in c) which performs \n",
    "    gradient descent on the subintervals to find optimal nodes\n",
    "    \n",
    "    Input:\n",
    "    numberOfSubintervals = number of subintervals in which to divide the interval\n",
    "    l = step size parameter\n",
    "    down = hyperparameter for reducing step length\n",
    "    up = hyperparameter for increasing step length\n",
    "    interval = interval of the function\n",
    "    function = function under consideration\n",
    "    \n",
    "    Output:\n",
    "    solution = an array of interpolated values\n",
    "    functionPoints = an array of the nodes in each subinterval\n",
    "    infNorm(...) = the max norm of the method\n",
    "    twoNorm(...) = the two norm of the method\n",
    "    \"\"\"\n",
    "    solution = np.array([])\n",
    "    functionPoints = np.array([])\n",
    "    subIntervals = np.linspace(interval[0], interval[1], numberOfSubintervals)\n",
    "    for i in range(1, len(subIntervals)):\n",
    "        #x = phi([subIntervals[i-1],subIntervals[i]], chebyshev(nodes+1))\n",
    "        \n",
    "        #Generate nodes\n",
    "        x = np.linspace(subIntervals[i-1],subIntervals[i],nodes+1)\n",
    "        \n",
    "        #Generate known function points\n",
    "        eta = np.linspace(interval[0], interval[1], 100)\n",
    "        \n",
    "        #Apply gradient descent\n",
    "        x, _, _ = project_gd(x, eta, l, down, up, [subIntervals[i-1], subIntervals[i]] , function, 20, 1e-5)      \n",
    "        \n",
    "        #Apply function to optimised nodes\n",
    "        y = function(x)\n",
    "        \n",
    "        #Interpolate\n",
    "        interpolation = lagInterp(x, y, np.linspace(subIntervals[i-1],subIntervals[i],nodes*10))\n",
    "        subIntPoints = np.linspace(subIntervals[i-1],subIntervals[i],nodes*10)\n",
    "        solution = np.append(solution, interpolation)\n",
    "        functionPoints = np.append(functionPoints, subIntPoints)\n",
    "    return solution, functionPoints, infNorm(function(functionPoints), solution), twoNorm(function(functionPoints), solution, interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing with mildly ill conditioned starting points\n",
    "x = np.linspace(-1, 1, 6)\n",
    "eta = np.linspace(0.5,1,100)\n",
    "l = 1\n",
    "down =1.5\n",
    "up = 0.9\n",
    "ill_conditioned, grad_val, costs = project_gd(x, eta, l, down, up, [-1,1], runge)\n",
    "ill_conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing with ill conditioned starting points\n",
    "x = np.linspace(-1, 1, 5)\n",
    "eta = np.linspace(-2,2,100)\n",
    "l = 1\n",
    "down =1.5\n",
    "up = 0.9\n",
    "ill_conditioned, grad_val, costs = project_gd(x, eta, l, down, up, [-1,1], runge)\n",
    "ill_conditioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run with good parameters\n",
    "x = np.linspace(-1, 1, 5)\n",
    "eta = np.linspace(-1,1,1000)\n",
    "l = 1\n",
    "down =1.5\n",
    "up = 0.9\n",
    "x, grad_val, costs = project_gd(x, eta, l, down, up, [-1,1], runge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(grad_val)\n",
    "plt.title(\"Gradient Norm\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Gradient norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(costs)\n",
    "plt.title(\"Cost function\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Value of cost function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the interpolated function versus the true function\n",
    "\n",
    "x_true = np.linspace(-1,1,1000)\n",
    "y_true = runge(x_true)\n",
    "plt.plot(x_true, y_true, 'r', label=\"True function\")\n",
    "plt.plot(eta, interp(x, eta, runge), 'b', label = \"Interpolation - gradient descent\")\n",
    "plt.title(\"Interpolated function versus true function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the max norm and the two norm of piecewise interpolation with gradient descent\n",
    "n_vals = np.arange(2, 11)\n",
    "l = 1\n",
    "down =1.5\n",
    "up = 0.9\n",
    "i_norm_list = np.array([])\n",
    "t_norm_list = np.array([])\n",
    "for n in n_vals:\n",
    "    _, _, iNorm, tNorm = pieceWise_gd(8, l, down, up, [-1,1], runge, n)\n",
    "    i_norm_list = np.append(i_norm_list, iNorm)\n",
    "    t_norm_list = np.append(t_norm_list, tNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(n_vals, i_norm_list, 'r', label =\"Max norm\")\n",
    "plt.semilogy(n_vals, t_norm_list, 'b', label =\"Two norm\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.title(\"Error estimates for piecewise interpolation with gradient descent\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Error estimates\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the max norm and the two norm of piecewise interpolation with gradient descent\n",
    "#and comparing them the standard piecewise method\n",
    "n_vals = np.arange(2, 11)\n",
    "l = 1\n",
    "down =1.5\n",
    "up = 0.9\n",
    "i_norm_list = np.array([])\n",
    "t_norm_list = np.array([])\n",
    "i_norm_list_gd = np.array([])\n",
    "t_norm_list_gd = np.array([])\n",
    "for n in n_vals:\n",
    "    _, _, max_norm, two_norm = pieceWise(8, [-1,1], runge, n)\n",
    "    i_norm_list = np.append(i_norm_list, max_norm)\n",
    "    t_norm_list = np.append(t_norm_list, two_norm)\n",
    "    \n",
    "    _, _, iNorm, tNorm = pieceWise_gd(8, l, down, up, [-1,1], runge, n)\n",
    "    i_norm_list_gd = np.append(i_norm_list_gd, iNorm)\n",
    "    t_norm_list_gd = np.append(t_norm_list_gd, tNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(n_vals, i_norm_list, 'r', label =\"Max norm std\")\n",
    "plt.semilogy(n_vals, t_norm_list, 'b', label =\"Two norm std\")\n",
    "plt.semilogy(n_vals, i_norm_list_gd, 'g', label =\"Max norm gd\")\n",
    "plt.semilogy(n_vals, t_norm_list_gd, 'purple', label =\"Two norm gd\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Error estimates\")\n",
    "#plt.savefig(path+\"piecewise_gd_error_project_vs_std\", bbox_extra_artists=[legend,], bbox_inches='tight')\n",
    "#We see that gradient descent performs poorly compared to the standard method and so will not be\n",
    "#investigated further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing all the methods so far for different values of n using the Runge function\n",
    "n_vals = np.arange(2, 11, 1)\n",
    "\n",
    "#Arrays tracking the error in max norm of the algorithms\n",
    "error_equidistant=np.array([])\n",
    "error_chebishev = np.array([])\n",
    "error_optimised = np.array([])\n",
    "error_piecewise = np.array([])\n",
    "\n",
    "#Arrays for tracking the amount of time spent on each algorithm\n",
    "time_gd = np.array([])\n",
    "time_lagrangian = np.array([])\n",
    "time_chebyshev = np.array([])\n",
    "time_piecewise = np.array([])\n",
    "\n",
    "#Value at which the function is known, i.e. f(eta)\n",
    "eta = np.linspace(-1, 1, 1000)\n",
    "\n",
    "#Step size parameters\n",
    "l = 1\n",
    "down =1.5\n",
    "up = 0.9\n",
    "for n in n_vals:\n",
    "    print(\"This is n\", n)\n",
    "    \n",
    "    #Generating nodes\n",
    "    x = np.linspace(-1,1,n)\n",
    "    cheb = chebyshev(n, -1, 1)\n",
    "    \n",
    "    #Points at which to evaluate the function\n",
    "    evalu = np.linspace(-1,1, 100)\n",
    "    \n",
    "    #Gradient descent\n",
    "    gd = time.time()\n",
    "    optimal, _, _= project_gd(x, eta, l, down, up, [-1,1], runge)\n",
    "    interp_optimal = lagInterp(optimal, runge(optimal), evalu)\n",
    "    time_gd = np.append(time_gd, time.time()-gd)\n",
    "    \n",
    "    #Lagrangian interpolation on equidistant nodes\n",
    "    lagr = time.time()\n",
    "    interp_equidistant = lagInterp(x, runge(x), evalu)\n",
    "    time_lagrangian = np.append(time_lagrangian, time.time()- lagr)\n",
    "    \n",
    "    #Lagrangian interpolation on chebyshev nodes\n",
    "    cheby = time.time()\n",
    "    interp_chebishev = lagInterp(cheb, runge(cheb), evalu)\n",
    "    time_chebyshev = np.append(time_chebyshev, time.time()-cheby)\n",
    "        \n",
    "    \n",
    "    #Piecewise interpolation\n",
    "    piece = time.time()\n",
    "    _, _, piece_norm, _ = pieceWise(k, [-1,1], runge, n)\n",
    "    time_piecewise = np.append(time_piecewise, time.time() - piece)\n",
    "    \n",
    "    error_equidistant=np.append(error_equidistant, twoNorm(runge(evalu), interp_equidistant, [-1,1]))\n",
    "    error_chebishev =np.append(error_chebishev, twoNorm(runge(evalu), interp_chebishev, [-1,1]))\n",
    "    error_optimised =np.append(error_optimised, twoNorm(runge(evalu), interp_optimal, [-1,1]))\n",
    "    error_piecewise =np.append(error_piecewise, piece_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing error estimates of the various methods\n",
    "\n",
    "plt.semilogy(n_vals, error_equidistant, 'r', label = \"Equidistant nodes\")\n",
    "plt.semilogy(n_vals, error_chebishev,'g', label = \"Chebyshev nodes\")\n",
    "plt.semilogy(n_vals, error_optimised, 'b', label=\"Optimised nodes\")\n",
    "plt.semilogy(n_vals, error_piecewise, 'orange', label=\"Piecewise\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel(\"n=\")\n",
    "plt.ylabel(\"Two norm\")\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing the time it takes to run for the various methods.\n",
    "\n",
    "plt.semilogy(n_vals, time_lagrangian, 'r', label = \"Equidistant nodes\")\n",
    "plt.semilogy(n_vals, time_chebyshev,'g', label = \"Chebyshev nodes\")\n",
    "plt.semilogy(n_vals, time_gd, 'b', label=\"Optimised nodes\")\n",
    "plt.semilogy(n_vals, time_piecewise, 'orange', label=\"Piecewise\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel(\"n=\")\n",
    "plt.ylabel(\"Two norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare results for different values of N for the known function values\n",
    "#We have chosen n=6 as this is the highest value for n as a compromise between computation time and performance\n",
    "\n",
    "\n",
    "#Arrays tracking the error in max norm of the algorithms\n",
    "error_optimised = np.array([])\n",
    "gradients = np.array([])\n",
    "cost_list = np.array([])\n",
    "\n",
    "#Arrays for tracking the amount of time spent on each algorithms\n",
    "time_gd = np.array([])\n",
    "\n",
    "\n",
    "large_n = np.array([10, 100, 1000, 2000, 5000])\n",
    "\n",
    "for et in large_n:\n",
    "    n=6\n",
    "    \n",
    "    #Known function values\n",
    "    eta = np.linspace(-1,1,et)\n",
    "    print(\"This is N\", et)\n",
    "    \n",
    "    #Generating nodes\n",
    "    x = np.linspace(-1,1,n)\n",
    "    cheb = chebyshev(n, -1, 1)\n",
    "    \n",
    "    #Points at which to evaluate the function\n",
    "    evalu = np.linspace(-1,1, 100)\n",
    "    \n",
    "    #Gradient descent\n",
    "    gd = time.time()\n",
    "    optimal, grads, costs= project_gd(x, eta, l, down, up, [-1,1], runge)\n",
    "    gradients = np.append(gradients, grads)\n",
    "    cost_list = np.append(cost_list, costs)\n",
    "    interp_optimal = lagInterp(optimal, runge(optimal), evalu)\n",
    "    time_gd = np.append(time_gd, time.time()-gd)\n",
    "   \n",
    "    error_optimised =np.append(error_optimised, twoNorm(runge(evalu), interp_optimal, [-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(large_n, error_optimised, 'b', label=\"Optimised nodes\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel(\"n=\")\n",
    "plt.ylabel(\"Two norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We explore the range between 10 and 1000 in greater detail\n",
    "#Arrays tracking the error in max norm of the algorithms\n",
    "error_optimised = np.array([])\n",
    "gradients = np.array([])\n",
    "cost_list = np.array([])\n",
    "\n",
    "#Arrays for tracking the amount of time spent on each algorithms\n",
    "time_gd = np.array([])\n",
    "\n",
    "\n",
    "large_n = np.append(np.array([10]), np.arange(20, 110, 10))\n",
    "large_n = np.append(large_n, np.arange(200, 1200, 200))\n",
    "\n",
    "for et in large_n:\n",
    "    n=6\n",
    "    \n",
    "    #Known function values\n",
    "    eta = np.linspace(-1,1,et)\n",
    "    print(\"This is N\", et)\n",
    "    \n",
    "    #Generating nodes\n",
    "    x = np.linspace(-1,1,n)\n",
    "    cheb = chebyshev(n, -1, 1)\n",
    "    \n",
    "    #Points at which to evaluate the function\n",
    "    evalu = np.linspace(-1,1, 100)\n",
    "    \n",
    "    #Gradient descent\n",
    "    gd = time.time()\n",
    "    optimal, grads, costs= project_gd(x, eta, l, down, up, [-1,1], runge)\n",
    "    gradients = np.append(gradients, grads)\n",
    "    cost_list = np.append(cost_list, costs)\n",
    "    interp_optimal = lagInterp(optimal, runge(optimal), evalu)\n",
    "    time_gd = np.append(time_gd, time.time()-gd)\n",
    "   \n",
    "    error_optimised =np.append(error_optimised, twoNorm(runge(evalu), interp_optimal, [-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(large_n, error_optimised, 'b', label=\"Optimised nodes\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel(\"n=\")\n",
    "plt.ylabel(\"Two norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(large_n, time_gd, 'b', label=\"Optimised nodes\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel(\"n=\")\n",
    "plt.ylabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We explore the range between 10 and 150 in greater detail\n",
    "#Arrays tracking the error in max norm of the algorithms\n",
    "error_optimised = np.array([])\n",
    "gradients = np.array([])\n",
    "cost_list = np.array([])\n",
    "\n",
    "#Arrays for tracking the amount of time spent on each algorithms\n",
    "time_gd = np.array([])\n",
    "\n",
    "\n",
    "large_n = np.append(np.array([10]), np.arange(20, 160, 10))\n",
    "l = 1\n",
    "down =1.5\n",
    "up = 0.9\n",
    "for et in large_n:\n",
    "    n=10\n",
    "    \n",
    "    #Known function values\n",
    "    eta = np.linspace(-1,1,et)\n",
    "    print(\"This is N\", et)\n",
    "    \n",
    "    #Generating nodes\n",
    "    x = np.linspace(-1,1,n)\n",
    "    cheb = chebyshev(n, -1, 1)\n",
    "    \n",
    "    #Points at which to evaluate the function\n",
    "    evalu = np.linspace(-1,1, 100)\n",
    "    \n",
    "    #Gradient descent\n",
    "    gd = time.time()\n",
    "    optimal, grads, costs= project_gd(x, eta, l, down, up, [-1,1], runge)\n",
    "    gradients = np.append(gradients, grads)\n",
    "    cost_list = np.append(cost_list, costs)\n",
    "    interp_optimal = lagInterp(optimal, runge(optimal), evalu)\n",
    "    time_gd = np.append(time_gd, time.time()-gd)\n",
    "   \n",
    "    error_optimised =np.append(error_optimised, twoNorm(runge(evalu), interp_optimal, [-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(large_n, error_optimised, 'b', label=\"Optimised nodes\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel(\"n=\")\n",
    "plt.ylabel(\"Two norm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(large_n, time_gd, 'b', label=\"Optimised nodes\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel(\"n=\")\n",
    "plt.ylabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi(x, shape):\n",
    "    return np.exp(-((shape*x)**2))\n",
    "\n",
    "def f_aprox(x, shape, eta, function):\n",
    "    \"\"\"\n",
    "    An interpolation function based on radial basis functions\n",
    "    \n",
    "    Input: \n",
    "    x = an array of nodes\n",
    "    shape = a shape parameter\n",
    "    eta = an array of values in which to evaluate the interpolation\n",
    "    function = the function under consideration\n",
    "    \n",
    "    Output:\n",
    "    approx = An interpolion of the function at the values of eta\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = np.array([])\n",
    "    for i in range(len(x)):\n",
    "        mi = np.array([])\n",
    "        for j in range(len(x)):\n",
    "            mi = np.append(mi, psi(np.abs(x[i]-x[j]), shape))\n",
    "        if(i==0):\n",
    "            m = mi\n",
    "        else:\n",
    "            m = np.vstack((m, mi))\n",
    "    f = function(x)\n",
    "    w = np.linalg.solve(m,f)\n",
    "    approx = 0\n",
    "    for i in range(len(x)):\n",
    "        approx += w[i]*psi(np.abs(eta-x[i]), shape)\n",
    "    return approx\n",
    "\n",
    "\n",
    "\n",
    "def cost2(x, shape, eta, interval, function):\n",
    "    \"\"\"\n",
    "    The cost function for RBF interpolation\n",
    "    \n",
    "    Input: \n",
    "    x = an array of nodes\n",
    "    shape = a shape parameter\n",
    "    eta = an array of values in which to evaluate the interpolation\n",
    "    function = the function under consideration\n",
    "    \n",
    "    Output:\n",
    "    the cost with the current set of nodes \n",
    "    \n",
    "    \"\"\"\n",
    "    func_eta = function(eta)\n",
    "    aprox = f_aprox(x, shape, eta, function)\n",
    "    aprox = np.sum((func_eta-aprox)**2)\n",
    "    return ((interval[1]-interval[0])/len(eta))*aprox\n",
    "\n",
    "  \n",
    "def f_3(x):\n",
    "    \"\"\"\n",
    "    The third function given in the project description\n",
    "    \"\"\"\n",
    "    return 0.75*(np.exp((-(9*x-2)**2)/4)+np.exp((-(9*x+1)**2)/49))+0.5*np.exp((-(9*x-7)**2)/4)-0.1*np.exp(-(9*x-4)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_rbf(numberOfSubintervals, interval, function, shape, nodes=5):\n",
    "    \"\"\"\n",
    "    A piecewise implementation of RBF interpolation.\n",
    "    \n",
    "    Input:\n",
    "    numberOfSubintervals = number of subintervals in which to divide the interval\n",
    "    interval = interval of the function\n",
    "    function = function under consideration\n",
    "    shape = shape parameter for RBF interpolation\n",
    "    \n",
    "    Output:\n",
    "    solution = an array of interpolated values\n",
    "    functionPoints = an array of the nodes in each subinterval\n",
    "    infNorm(...) = the max norm of the method\n",
    "    twoNorm(...) = the two norm of the method\n",
    "    \"\"\"\n",
    "    \n",
    "    solution = np.array([])\n",
    "    functionPoints = np.array([])\n",
    "    subIntervals = np.linspace(interval[0], interval[1], numberOfSubintervals)\n",
    "    for i in range(1, len(subIntervals)):\n",
    "        x = np.linspace(subIntervals[i-1],subIntervals[i],nodes+1)\n",
    "        eta = np.linspace(subIntervals[i-1],subIntervals[i], nodes*10)\n",
    "        interpolation = f_aprox(x, shape, eta, function)\n",
    "        subIntPoints = np.linspace(subIntervals[i-1],subIntervals[i],nodes*10)\n",
    "        solution = np.append(solution, interpolation)\n",
    "        functionPoints = np.append(functionPoints, subIntPoints)\n",
    "    return solution, functionPoints, infNorm(function(functionPoints), solution), twoNorm(function(functionPoints), solution, interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing if our radial basis function works\n",
    "x_true = np.linspace(-1,1,1000)\n",
    "y_true = runge(x_true)\n",
    "\n",
    "x = np.linspace(-1,1,10)\n",
    "eta = np.linspace(-1,1,1000)\n",
    "\n",
    "plt.plot(x_true, y_true, 'r', label = \"True function\")\n",
    "plt.plot(eta, f_aprox(x, 1.5, eta, runge), 'b', label = \"RBF interpolation\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true = np.linspace(-1,1,1000)\n",
    "y_true = f_3(x_true)\n",
    "\n",
    "x = np.linspace(-1,1,100)\n",
    "eta = np.linspace(-1,1,100)\n",
    "\n",
    "plt.plot(x_true, y_true, 'r', label = \"True function\")\n",
    "plt.plot(eta, f_aprox(x, 1.5, eta, f_3), 'b', label = \"RBF interpolation\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "legend = plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at how n affects the 2 norm for the runge function\n",
    "norms_equi = np.array([])\n",
    "norms_cheb = np.array([])\n",
    "\n",
    "eta = np.linspace(-1,1,1000)\n",
    "shape = 2\n",
    "nVals = np.arange(2, 50, 1)\n",
    "for n in nVals:\n",
    "    x = np.linspace(-1,1,n)\n",
    "    y = chebyshev(n,-1,1)\n",
    "    norms_equi = np.append(norms_equi, twoNorm(runge(eta), f_aprox(x, shape, eta, runge), [-1,1]))\n",
    "    norms_cheb = np.append(norms_cheb, twoNorm(runge(eta), f_aprox(y, shape, eta, runge), [-1,1]))\n",
    "plt.semilogy(nVals, norms_equi, 'r', nVals, norms_cheb, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at how the shape parameter affects the interpolation error for the runge function\n",
    "norms_equi = np.array([])\n",
    "norms_cheb = np.array([])\n",
    "x = np.linspace(-1,1,10)\n",
    "y = chebyshev(10,-1,1)\n",
    "eta = np.linspace(-1,1,1000)\n",
    "for shape in np.linspace(0.5, 10.0, 50):\n",
    "    norms_equi = np.append(norms_equi, twoNorm(runge(eta), f_aprox(x, shape, eta, runge), [-1,1]))\n",
    "    norms_cheb = np.append(norms_cheb, twoNorm(runge(eta), f_aprox(y, shape, eta, runge), [-1,1]))\n",
    "plt.semilogy(np.linspace(0.5, 10.0, 50), norms_equi, 'r', np.linspace(0.5, 10.0, 50), norms_cheb, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at how n affects the 2 norm for f_3\n",
    "norms_equi = np.array([])\n",
    "norms_cheb = np.array([])\n",
    "\n",
    "eta = np.linspace(-1,1,1000)\n",
    "shape = 2\n",
    "nVals = np.arange(2, 50, 1)\n",
    "for n in nVals:\n",
    "    x = np.linspace(-1,1,n)\n",
    "    y = chebyshev(n,-1,1)\n",
    "    norms_equi = np.append(norms_equi, twoNorm(runge(eta), f_aprox(x, shape, eta, f_3), [-1,1]))\n",
    "    norms_cheb = np.append(norms_cheb, twoNorm(runge(eta), f_aprox(y, shape, eta, f_3), [-1,1]))\n",
    "plt.semilogy(nVals, norms_equi, 'r', nVals, norms_cheb, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Looking at how the shape parameter affects the interpolation error for f_3 with 10 nodes\n",
    "norms_equi = np.array([])\n",
    "norms_cheb = np.array([])\n",
    "x = np.linspace(-1,1,10)\n",
    "y = chebyshev(10, -1, 1)\n",
    "eta = np.linspace(-1,1,2000)\n",
    "for shape in np.linspace(0.5, 10.0, 50):\n",
    "    norms_equi = np.append(norms_equi, twoNorm(f_3(eta), f_aprox(x, shape, eta, f_3), [-1,1]))\n",
    "    norms_cheb = np.append(norms_cheb, twoNorm(f_3(eta), f_aprox(y, shape, eta, f_3), [-1,1]))\n",
    "plt.semilogy(np.linspace(0.5, 10.0, 50), norms_equi, 'r', np.linspace(0.5, 10.0, 50), norms_cheb, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent2(x, shape, eta, interval, function, max_iter = 100, tol = 1e-15):\n",
    "    \"\"\"\n",
    "    Gradient descent with armijo backtracking\n",
    "    \n",
    "    Input:\n",
    "    x = an array of nodes\n",
    "    shape = shape parameter for RBF interpolation\n",
    "    eta = an array of values at which to evaluate the interpolation\n",
    "    interval = the interval in which we evaluate the function\n",
    "    function = the function we wish to interpolate\n",
    "    max_iter = maximum number of iterations after which we return the current solution\n",
    "    tol = tolerance which we compare the norm of the gradient to to determine when we exit the algorithm\n",
    "    \n",
    "    Output:\n",
    "    x = an array of nodes\n",
    "    cost_list = a list of the values of the cost function at each iteration\n",
    "    norm_list = a list of the norm of the gradient of the cost function at each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    #Setting initial values and lists\n",
    "    x = x\n",
    "    shape = shape\n",
    "    c = 1e-4\n",
    "    gradient = grad(cost2,(0,1))\n",
    "    x, shape, eta, interval, function\n",
    "    combined = gradient(x,shape, eta, interval, function)\n",
    "    p_k = -np.append(combined[0], combined[1])\n",
    "    norm_p_k = np.linalg.norm(p_k)\n",
    "    grad_norm_list = np.array([norm_p_k])\n",
    "    costs = np.array(cost2(x, shape, eta, interval, function))\n",
    "    iterations_at_minima = 0\n",
    "    \n",
    "    while max_iter>0 and norm_p_k>tol:\n",
    "        #print(\"We are on iteration\", max_iter)\n",
    "        #print(\"This is shape\", shape)\n",
    "        #print(\"This is p_k\", p_k)\n",
    "        #print(\"This is the norm\", norm_p_k)\n",
    "        #print(\"This is x\", x)\n",
    "        \n",
    "        #Initial step length parameter. Testing has shown that\n",
    "        #starting with a high one increases convergence speed\n",
    "        #but risks jumping outside the x range we desire.\n",
    "        #Higher node counts seem to benefit from longer steps.\n",
    "        alpha = (len(x)**7)\n",
    "        \n",
    "        \n",
    "        #Armijo condition\n",
    "        while cost2(x+alpha*p_k[:-1],shape + alpha*p_k[-1], eta, interval, function)>(cost2(x, shape, eta, interval, function)+c*alpha*(-p_k@p_k)):\n",
    "            #print(\"We are on alpha\", alpha)\n",
    "            \n",
    "            #In case alpha is too small\n",
    "            if(alpha<1e-25):\n",
    "                break\n",
    "            alpha = 0.5*alpha\n",
    "            \n",
    "        #print(\"This alpha got through\", alpha)\n",
    "        \n",
    "        #Set new values after taking a step\n",
    "        x = x + alpha*p_k[:-1]\n",
    "        shape = shape + alpha*p_k[-1]\n",
    "        combined = gradient(x,shape, eta, interval, function)\n",
    "        p_k = -np.append(combined[0], combined[1])\n",
    "        norm_p_k = np.linalg.norm(p_k)\n",
    "        max_iter -= 1\n",
    "        \n",
    "        #Adds to lists\n",
    "        grad_norm_list = np.append(grad_norm_list, norm_p_k)\n",
    "        costs = np.append(costs, cost2(x, shape, eta, interval, function))\n",
    "        \n",
    "        #Stopping criteria in case progression stalls\n",
    "        if (np.abs(grad_norm_list[-1]-grad_norm_list[-2])<1e-20):\n",
    "            print(\"Delta grad is\", grad_norm_list[-1]-grad_norm_list[-2])\n",
    "            print(\"Delta grad is smaller than 1e-17\")\n",
    "            if(iterations_at_minima >5):\n",
    "                return x, shape, grad_norm_list, costs\n",
    "            else:\n",
    "                iterations_at_minima += 1\n",
    "        \n",
    "    return x, shape, grad_norm_list, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing our algorithm with n=6 nodes and 30 iterations\n",
    "\n",
    "x = np.linspace(-1,1,6)\n",
    "shape = 2.0\n",
    "eta = np.linspace(-1,1,1500)\n",
    "x, shape, grad_norm_list, costs = gradient_descent2(x, shape, eta, [-1,1], runge, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(grad_norm_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eta, f_aprox(x, shape, eta, runge), 'r', label = \"Interpolated function\")\n",
    "plt.plot(eta, runge(eta), 'b', label = \"True function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "import  numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.polynomial.polynomial as poly\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "\n",
    "def f1(x):\n",
    "    y=1/(1+x**2)\n",
    "    return y\n",
    "\n",
    "def f2(x):\n",
    "    y=np.cos(2*np.pi*x)\n",
    "    return y\n",
    "\n",
    "def f3(x):\n",
    "    y=np.exp(3*x)*np.sin(2*x)\n",
    "    return y\n",
    "\n",
    "def f4(x):\n",
    "    y=np.sinc(x)\n",
    "    return y\n",
    "\n",
    "def f5(x):\n",
    "    y=3/4*(np.exp(-((9*x-2)**2)/4)+np.exp(((9*x+1)**2)/49))\n",
    "    y=y+1/2*np.exp(-((9*x-7)**2)/4)-1/10*np.exp(-(9*x-4)**2)\n",
    "    return y\n",
    "\n",
    "def interpolate(x, x_values, y_values):\n",
    "    def _basis(j):\n",
    "        p=1\n",
    "        for m in range(0,k):\n",
    "            if m !=j:\n",
    "               p = p*(x - x_values[m])/(x_values[j] - x_values[m]) \n",
    "        return p\n",
    "    assert len(x_values) != 0 and (len(x_values) == len(y_values)), 'x and y cannot be empty and must have the same length'\n",
    "    k = len(x_values)\n",
    "    return sum(_basis(j)*y_values[j] for j in range(k))\n",
    "\n",
    "def chebychev(n, a, b): \n",
    "    j = np.arange(n+1)\n",
    "    x = np.cos((j + 0.5) / (n + 1) * np.pi)\n",
    "    return (b - a) / 2 * x + (b + a) / 2\n",
    "\n",
    "def gd(x, tau, maxiter, D_C): \n",
    "    for k in range(maxiter):\n",
    "        x=x-tau*D_C(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  autograd.numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "\n",
    "def C_factory(a, b, N, f, xfine, yvaluesf):\n",
    "    \n",
    "    def C(x):\n",
    "        yfine=interpolate(xfine,x,f(x))\n",
    "        return (b-a)/N*np.sum(np.square(yvaluesf-yfine))\n",
    "    \n",
    "    return C\n",
    "           \n",
    "# Dataset\n",
    "a = 0\n",
    "b = 1\n",
    "N = 1000\n",
    "f = f5\n",
    "x_data = np.linspace(a, b, num=N+1)   \n",
    "y_data = f(x_data)\n",
    "\n",
    "C = C_factory(a,b,N,f,x_data,y_data)          \n",
    "D_C   = grad(C)    \n",
    "\n",
    "# Optimise           \n",
    "n = 4\n",
    "maxiter=1000\n",
    "tau=0.01\n",
    "x = np.linspace(a,b,n+1)\n",
    "\n",
    "print(\"Initial cost\",np.sqrt(C(x)))\n",
    "x_phi = gd(x, tau, maxiter, D_C)\n",
    "print(\"Final cost\",np.sqrt(C(x_phi)))\n",
    "\n",
    "x_cheb = chebychev(n, a, b)\n",
    "ycheb = f(x_cheb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data, f(x_data),'k', label = 'real')\n",
    "plt.plot(x_data, interpolate(x_data,x_phi,f(x_phi)),'g', label = 'opt_int')\n",
    "plt.plot(x_data, interpolate(x_data,x_cheb,f(x_cheb)), 'r', label = 'cheb')\n",
    "# plt.plot(x0, f(x0),'bs')\n",
    "# plt.plot(xfine, interpolate(xfine,x0,f(x0)),'b')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(r, ep):\n",
    "    return np.exp(-(ep*r)**2)\n",
    "\n",
    "def rbf(xi, ep, x_values, y_values):    \n",
    "    def rbf_basis(j):\n",
    "        return phi(np.abs(xi - x_values[j]), ep)  \n",
    "    \n",
    "    assert len(x_values) != 0 and (len(x_values) == len(y_values)), 'x and y cannot be empty and must have the same length'\n",
    "    k = len(x_values)\n",
    "    e = np.ones((k,))\n",
    "    D = np.outer(x_values, e) - np.outer(e, x_values)\n",
    "    M = phi(np.abs(D), ep)\n",
    "    w = np.linalg.solve(M, y_values)\n",
    "    return sum(rbf_basis(j) * w[j] for j in range(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  autograd.numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "\n",
    "\n",
    "def C_factory(a, b, N, f, xfine, yvaluesf):\n",
    "    \n",
    "    def C(xep):\n",
    "        n1=len(xep)\n",
    "        x=xep[0:n1-2]\n",
    "        ep=xep[n1-1]\n",
    "        ytildefine = rbf(xfine,ep,x,f(x))\n",
    "        \n",
    "        return (b-a)/N*sum(np.square(yvaluesf-ytildefine))\n",
    "    \n",
    "    return C\n",
    "\n",
    "# Dataset\n",
    "a = 0\n",
    "b = 1\n",
    "N = 1000\n",
    "f = f5\n",
    "\n",
    "xfine = np.linspace(a, b, num=N+1)   \n",
    "h=(b-a)/N\n",
    "yvaluesf = f(xfine)\n",
    "\n",
    "\n",
    "C = C_factory(a,b,N,f,xfine,yvaluesf)     \n",
    "D_C   = grad(C)  \n",
    "\n",
    "# Optimise           \n",
    "n = 5\n",
    "x0 = np.linspace(a,b,n+1)\n",
    "#x = np.random.rand(n+1)\n",
    "#x = a + x* (b-a)\n",
    "\n",
    "xep=np.zeros(n+2)\n",
    "\n",
    "xep[0:n]=x0[0:n]\n",
    "xep[n+1]=1.\n",
    "print(xep)\n",
    "\n",
    "maxiter=100\n",
    "\n",
    "ci = 0.9\n",
    "ci1 = 2\n",
    "L = 10.0\n",
    "tau=0.001\n",
    "\n",
    "Cx=C(xep)\n",
    "CC=np.zeros(maxiter+1)\n",
    "\n",
    "print(\"Initial cost\",np.sqrt(Cx))\n",
    "CC[0]=Cx\n",
    "for k in range(maxiter):\n",
    "    gC = D_C(xep)\n",
    "    for ell in range(200):\n",
    "        alpha = 1./L\n",
    "        xepn = xep-alpha*gC\n",
    "        Cxn=C(xepn) \n",
    "        m = np.inner(-alpha*gC,gC)\n",
    "        m1 = L/2* np.inner(alpha*gC,alpha*gC)\n",
    "        if (Cxn <= Cx + 0.4*m ):         \n",
    "           # print(np.linalg.norm(xep-xepn))\n",
    "            xep = xepn \n",
    "            Cx = Cxn\n",
    "            L = ci *L\n",
    "            break        \n",
    "        else:\n",
    "            L = ci1 *L\n",
    "#                    \n",
    "    CC[k+1]=Cx\n",
    "    \n",
    "    if ((np.abs(CC[k+1]-CC[k])<=10**-6) & (np.abs(m/alpha)<=10**-6)):\n",
    "        #print(np.abs(m/alpha))\n",
    "        #print(np.abs(CC[k+1]-CC[k]))\n",
    "        break\n",
    "    if (ell != 0):\n",
    "        print(ell)\n",
    "        print(CC[k+1]-CC[k])\n",
    "                \n",
    "           \n",
    "plt.semilogy(range(maxiter+1), CC,'k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.zeros(n+1)\n",
    "x[0:n]=xep[0:n]\n",
    "ep=xep[n+1]\n",
    "\n",
    "plt.plot(xfine, rbf(xfine,ep,x,f(x)),'r', label = 'rbf')\n",
    "plt.plot(xfine, f(xfine), label = 'analytical')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def C_factory(a, b, N, f, xfine, yvaluesf):\n",
    "    \n",
    "    def C(x):\n",
    "        yfine=interpolate(xfine,x,f(x))\n",
    "        return (b-a)/N*sum(np.square(yvaluesf-yfine))\n",
    "    \n",
    "    return C\n",
    "\n",
    "f = f5\n",
    "\n",
    "# Generate sample data\n",
    "N = 20\n",
    "x_data = np.linspace(0, 1, N)\n",
    "y_data = f(x_data)\n",
    "\n",
    "# Initialize coefficients for the polynomial (e.g., cubic polynomial)\n",
    "n = 10\n",
    "coeffs_initial = np.linspace(0,1,n + 1)\n",
    "\n",
    "# Create the gradient of the cost function\n",
    "cost = C_factory(0, 1, 100, f, x_data, y_data)\n",
    "# cost_gradient = grad(cost)\n",
    "\n",
    "# # Parameters for gradient descent\n",
    "# alpha = 0.01  # Learning rate\n",
    "# iterations = 1000  # Number of iterations\n",
    "\n",
    "# # Perform gradient descent\n",
    "# coeffs_optimal = coeffs_initial.copy()\n",
    "# for i in range(iterations):\n",
    "#     grad_values = cost_gradient(coeffs_optimal)\n",
    "#     coeffs_optimal -= alpha * grad_values  # Update rule\n",
    "\n",
    "coeffs_optimal = minimize(cost, coeffs_initial).x\n",
    "\n",
    "# Generate fine x values for plotting\n",
    "x_fine = np.linspace(0, 1, 100)\n",
    "y_fine = f(x_fine)\n",
    "y_poly_opt = interpolate(x_fine, coeffs_optimal, f(coeffs_optimal))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_fine, y_fine, label='Original Function', color='blue')\n",
    "plt.plot(x_fine, y_poly_opt, label='Optimized Polynomial Approximation', color='red', linestyle='--')\n",
    "plt.scatter(x_data, y_data, color='green', marker='o', label='Data Points')\n",
    "plt.title('Original Function vs. Polynomial Approximation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.interpolate import BarycentricInterpolator\n",
    "# Import RBFInterpolator\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "\n",
    "# type = 'cubic'\n",
    "# type = 'gaussian'\n",
    "# type = 'linear'\n",
    "# type = 'quintic'\n",
    "type = 'thin_plate_spline'\n",
    "\n",
    "def C_factory(a, b, N, f, xdense, yvaluesf):\n",
    "    \n",
    "    def C(x):\n",
    "        x_eq = np.linspace(a, b, len(x))\n",
    "        assert len(x_eq) == len(x), 'x and x_eq must have the same length'\n",
    "        # yfine = interpolate(xfine, x_eq, f(x))\n",
    "        # yfine = BarycentricInterpolator(x_eq, f(x))(xfine)\n",
    "\n",
    "        # x_eq = np.sort(x_eq)\n",
    "\n",
    "\n",
    "        # Fix dim for RBFInterpolator\n",
    "        x_eq = x_eq.reshape(-1, 1)\n",
    "        xfine = xdense.reshape(-1, 1)\n",
    "        y_at_x = f(x).reshape(-1, 1)\n",
    "\n",
    "        yfine = RBFInterpolator(x_eq, y_at_x, kernel = type)(xfine)\n",
    "        cost = (b-a)/N * np.sum(np.square(yvaluesf - yfine))\n",
    "\n",
    "        # # Make sure x is strictly increasing by adding a penalty\n",
    "        # differences = np.diff(x)\n",
    "        # penalty = np.sum(np.abs(np.clip(differences, None, 0)))\n",
    "        # cost = cost + penalty\n",
    "\n",
    "        return cost \n",
    "    \n",
    "    return C\n",
    "\n",
    "def f(x):\n",
    "    y=3/4*(np.exp(-((9*x-2)**2)/4)+np.exp(((9*x+1)**2)/49))\n",
    "    y=y+1/2*np.exp(-((9*x-7)**2)/4)-1/10*np.exp(-(9*x-4)**2)\n",
    "    y = y.reshape(-1, 1)\n",
    "    return y\n",
    "\n",
    "phi = lambda x: x**2\n",
    "\n",
    "# Generate data to fit against\n",
    "N = 1000\n",
    "x_data = np.linspace(0, 1, N)\n",
    "y_data = f(phi(x_data))\n",
    "\n",
    "# Initialize coefficients for the polynomial (e.g., cubic polynomial)\n",
    "n = 100\n",
    "coeffs_initial = np.linspace(0,1,n+1)\n",
    "\n",
    "cost = C_factory(0, 1, N, f, x_data, y_data)\n",
    "\n",
    "coeffs_optimal = minimize(cost, coeffs_initial, method = 'BFGS').x\n",
    "\n",
    "print(f\"The start cost: {cost(coeffs_initial)}\")\n",
    "print(f\"The end cost: {cost(coeffs_optimal)}\")\n",
    "\n",
    "# Generate fine x values for plotting\n",
    "x_fine = np.linspace(0, 1, 1000)\n",
    "y_fine = f(x_fine)\n",
    "\n",
    "# Interpolate the solution\n",
    "# y_poly_opt = interpolate(x_fine, coeffs_initial, f(coeffs_optimal))\n",
    "# y_poly_opt = BarycentricInterpolator(np.linspace(0, 1, len(coeffs_optimal)), f(coeffs_optimal))(x_fine)\n",
    "\n",
    "\n",
    "x_ = np.linspace(0, 1, len(coeffs_optimal)).reshape(-1, 1)\n",
    "# coeffs_optimal = np.sort(coeffs_optimal)\n",
    "x_opt = coeffs_optimal\n",
    "y_ = f(x_opt)\n",
    "x_d = x_fine.reshape(-1, 1)\n",
    "print(x_.shape, x_opt.shape, y_.shape, x_d.shape)\n",
    "                 \n",
    "y_poly_opt = RBFInterpolator(x_, y_, kernel = type)(x_d)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_fine, y_fine, label='Original Function', color='blue')\n",
    "plt.plot(x_fine, y_poly_opt, label='Optimized Polynomial Approximation', color='red', linestyle='--')\n",
    "plt.scatter(x_data, y_data, color='green', marker='o', label='Data Points')\n",
    "plt.title('Original Function vs. Polynomial Approximation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.interpolate import BarycentricInterpolator\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "from scipy.optimize import Bounds\n",
    "\n",
    "def derivative_constraint(x):\n",
    "    return np.diff(x)\n",
    "\n",
    "def C_factory(a, b, N, f, xfine, yvaluesf):\n",
    "    \n",
    "    def C(x):\n",
    "        x_eq = np.linspace(a, b, len(x))\n",
    "        assert len(x_eq) == len(x), 'x and x_eq must have the same length'\n",
    "        yfine = BarycentricInterpolator(x_eq, f(x))(xfine)\n",
    "        cost = (b-a)/N * np.sum(np.square(yvaluesf - yfine))\n",
    "\n",
    "        return cost \n",
    "    \n",
    "    return C\n",
    "\n",
    "def f(x):\n",
    "    y=3/4*(np.exp(-((9*x-2)**2)/4)+np.exp(((9*x+1)**2)/49))\n",
    "    y=y+1/2*np.exp(-((9*x-7)**2)/4)-1/10*np.exp(-(9*x-4)**2)\n",
    "    y = y.reshape(-1, 1)\n",
    "    return y\n",
    "\n",
    "phi = lambda x: x**2\n",
    "# phi = lambda x: np.sqrt(x)\n",
    "# phi = lambda x: x\n",
    "\n",
    "# Generate data to fit against\n",
    "N = 30\n",
    "x_data = np.linspace(0, 1, N)\n",
    "y_data = f(phi(x_data))\n",
    "\n",
    "# Initialize coefficients for the polynomial (e.g., cubic polynomial)\n",
    "n = 10\n",
    "coeffs_initial = np.linspace(0,1,n+1)\n",
    "cost = C_factory(0, 1, N, f, x_data, y_data)\n",
    "\n",
    "bounds = [(0, 0)] + [(None, None)] * (n-1) + [(1, 1)]\n",
    "nonlinear_constraint = NonlinearConstraint(derivative_constraint, 0, np.inf)\n",
    "coeffs_optimal = minimize(cost, coeffs_initial, method = 'SLSQP', bounds = bounds, constraints=[nonlinear_constraint]).x\n",
    "\n",
    "print(f\"The start cost: {cost(coeffs_initial)}\")\n",
    "print(f\"The end cost: {cost(coeffs_optimal)}\")\n",
    "\n",
    "# Generate fine x values for plotting\n",
    "x_fine = np.linspace(0, 1, 1000)\n",
    "y_fine = f(x_fine)\n",
    "\n",
    "y_poly_opt = BarycentricInterpolator(np.linspace(0, 1, len(coeffs_optimal)), f(coeffs_optimal))(x_fine)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_fine, y_fine, label='Original Function', color='blue')\n",
    "plt.plot(x_fine, y_poly_opt, label='Optimized Polynomial Approximation', color='red', linestyle='--')\n",
    "plt.scatter(x_data, y_data, color='green', marker='o', label='Data Points')\n",
    "# Plot vertical lines in coeffs_optimal\n",
    "\n",
    "plt.title('Original Function vs. Polynomial Approximation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(coeffs_initial, coeffs_optimal)\n",
    "plt.plot(coeffs_initial, phi(coeffs_initial))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use splines \n",
    "\n",
    "- Allows for more nodes to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nodes(x):\n",
    "    n = len(x) * 2 - 1  \n",
    "    x_new = np.linspace(x[0], x[-1], n)\n",
    "    return np.interp(x_new, x, x)\n",
    "\n",
    "def increasing_chebyshev_nodes(n, a, b):\n",
    "    j = np.arange(n+1)\n",
    "    x = np.cos((j + 0.5) / (n + 1) * np.pi)\n",
    "    nodes_decreasing = (b - a) / 2 * x + (b + a) / 2\n",
    "    return nodes_decreasing[::-1]\n",
    "\n",
    "print(increasing_chebyshev_nodes(5, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "def derivative_constraint(x):\n",
    "    return np.diff(x)\n",
    "\n",
    "def C_factory(a, b, N, f, xfine, yvaluesf):\n",
    "    \n",
    "    def C(x):\n",
    "        x_ = np.linspace(a, b, len(x))\n",
    "        # x_ = increasing_chebyshev_nodes(len(x) - 1, a, b)\n",
    "        assert len(x) == len(x_), f'x and x_eq must have the same length, {len(x)} != {len(x_)}'\n",
    "\n",
    "        spline = CubicSpline(x_, f(x))\n",
    "        yfine = spline(xfine)\n",
    "\n",
    "        cost = np.mean(np.square(f(phi(xfine)) - yfine))\n",
    "\n",
    "        reg = 0\n",
    "\n",
    "        # Smoothness of x_phi \n",
    "        reg += np.sum(np.diff(x) ** 2)\n",
    "\n",
    "        # Smoothness of the function\n",
    "        # reg +=  np.sum(spline(xfine, 2)**2) * 1e-8\n",
    "\n",
    "        return cost + reg * 0.1\n",
    "    \n",
    "    return C\n",
    "\n",
    "def f(x):\n",
    "    y=3/4*(np.exp(-((9*x-2)**2)/4)+np.exp(((9*x+1)**2)/49))\n",
    "    y=y+1/2*np.exp(-((9*x-7)**2)/4)-1/10*np.exp(-(9*x-4)**2)\n",
    "    y = y.reshape(-1, 1)\n",
    "    return y\n",
    "\n",
    "phi = lambda x: x**2\n",
    "# phi = lambda x: np.sqrt(x)\n",
    "# phi = lambda x: x\n",
    "\n",
    "# Generate data to fit against\n",
    "N = 100\n",
    "x_data = np.linspace(0, 1, N)\n",
    "y_data = f(phi(x_data))\n",
    "\n",
    "# Initialize coefficients for the polynomial (e.g., cubic polynomial)\n",
    "# n = 100\n",
    "# coeffs_initial = np.linspace(0,1,n+1)\n",
    "# coeffs_initial = np.linspace(0,1,len(x_data))\n",
    "x_init = np.linspace(0,1,100)\n",
    "cost = C_factory(0, 1, N, f, x_data, y_data)\n",
    "\n",
    "# bounds = Bounds(0, 1)\n",
    "n = len(x_init)\n",
    "bounds = [(0, 0)] + [(None, None)] * (n-2) + [(1, 1)]\n",
    "\n",
    "def increasing_constraint(x, i):\n",
    "    return x[i+1] - x[i]  # x[i+1] > x[i]\n",
    "\n",
    "constraints = [{'type': 'ineq', 'fun': increasing_constraint, 'args': (i,)} for i in range(n-1)]\n",
    "\n",
    "options = {\n",
    "    'disp': True,\n",
    "    'ftol': 1e-9,\n",
    "    'maxiter': 1000,  # Increase the number of iterations\n",
    "    'eps': 1.4901161193847656e-08  # Adjust the step size for numerical differentiation\n",
    "}\n",
    "\n",
    "result = minimize(cost, \n",
    "                  x_init, \n",
    "                  method = 'SLSQP', \n",
    "                  bounds = bounds, \n",
    "                  constraints=constraints,\n",
    "                  options=options,\n",
    "                  )\n",
    "\n",
    "x_opt = result.x\n",
    "\n",
    "print(f\"Converged: {result.success}\")\n",
    "print(f\"Message: {result.message}\")\n",
    "\n",
    "print(f\"The start cost: {cost(x_init)}\")\n",
    "print(f\"The end cost: {cost(x_opt)}\")\n",
    "print(f\"Optimal nodes cost: {cost(phi(x_init))}\")\n",
    "\n",
    "# Generate fine x values for plotting\n",
    "x_fine = np.linspace(0, 1, 1000)\n",
    "y_fine = f(x_fine)\n",
    "\n",
    "x_ = np.linspace(0, 1, len(x_opt))\n",
    "spline_opt = CubicSpline(x_, f(x_opt))\n",
    "y_poly_opt = spline_opt(x_fine)\n",
    "print(np.linalg.norm(y_poly_opt - f(phi(x_fine))))\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_fine, y_fine, label='Original Function', color='blue')\n",
    "plt.plot(x_fine, y_poly_opt, label='Optimized Polynomial Approximation', color='red', linestyle='--')\n",
    "plt.plot(phi(x_init), f(phi(x_init)), label = 'Opt')\n",
    "plt.scatter(x_data, y_data, color='green', marker='o', label='Data Points')\n",
    "# Plot vertical lines in coeffs_optimal\n",
    "\n",
    "plt.title('Original Function vs. Polynomial Approximation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_opt))\n",
    "plt.plot(x_init, x_opt)\n",
    "plt.plot(x_init, phi(x_init))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cubic Splines in R^3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "def derivative_constraint(x):\n",
    "    return np.diff(x)\n",
    "\n",
    "def C_one_dim_factory(f, xfine, phi):\n",
    "    \"\"\" \n",
    "    Assumes f is linear\n",
    "    \"\"\"\n",
    "    def C(x):\n",
    "        x_eq = np.linspace(0, 1, len(x))\n",
    "        spline = CubicSpline(x_eq, f(x))\n",
    "        yfine = spline(xfine)\n",
    "        y_approx = f(phi(xfine))\n",
    "        cost = np.mean(np.square(y_approx - yfine))\n",
    "        # diff = np.diff(x)\n",
    "        # reg = np.sum(np.where(diff < 0, diff, 0))\n",
    "        # reg = np.sum(np.diff(x) ** 2)\n",
    "        # reg = np.sum(np.diff(y_approx) ** 2)\n",
    "        return cost\n",
    "    return C\n",
    "\n",
    "\n",
    "def C_factory(f1, f2, f3, xfine, phi):\n",
    "    def C(x):\n",
    "        \"\"\"\n",
    "        f: R -> R^3\n",
    "        f = [f1, f2, f3]\n",
    "        Assumes f1, f2, f3 are linear\n",
    "        \"\"\"\n",
    "        c1 = C_one_dim_factory(f1, xfine, phi)\n",
    "        c2 = C_one_dim_factory(f2, xfine, phi)\n",
    "        c3 = C_one_dim_factory(f3, xfine, phi)\n",
    "        # c1_, c2_, c3_ = c1(x), c2(x), c3(x) \n",
    "        # return c1_ + c2_ + c3_\n",
    "        return c1(x) + c2(x) + c3(x)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_results(func1, func2, func3, phi, x_phi): \n",
    "    # Three subplots (1,3)\n",
    "    x = np.linspace(0, 1, len(x_phi))\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axs[0].plot(x, func1(phi(x)), label='Target', color='blue')\n",
    "    axs[0].plot(x, func1(x_phi), label='Interpolated', color='red', linestyle='--')\n",
    "    axs[0].set_title('f1')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(x, func2(phi(x)), label='Target', color='blue')\n",
    "    axs[1].plot(x, func2(x_phi), label='Interpolated', color='red', linestyle='--')\n",
    "    axs[1].set_title('f2')\n",
    "    axs[1].legend()\n",
    "\n",
    "    axs[2].plot(x, func3(phi(x)), label='Target', color='blue')\n",
    "    axs[2].plot(x, func3(x_phi), label='Interpolated', color='red', linestyle='--')\n",
    "    axs[2].set_title('f3')\n",
    "    axs[2].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    y=1/(1+x**2)\n",
    "    return y\n",
    "\n",
    "def f2(x):\n",
    "    y=np.cos(2*np.pi*x)\n",
    "    return y\n",
    "\n",
    "def f3(x):\n",
    "    y=np.exp(3*x)*np.sin(2*x)\n",
    "    return y\n",
    "\n",
    "def f4(x):\n",
    "    y=np.sinc(x)\n",
    "    return y\n",
    "\n",
    "def f5(x):\n",
    "    y=3/4*(np.exp(-((9*x-2)**2)/4)+np.exp(((9*x+1)**2)/49))\n",
    "    y=y+1/2*np.exp(-((9*x-7)**2)/4)-1/10*np.exp(-(9*x-4)**2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_nodes(x_init, cost, niter = 10):\n",
    "    n = len(x_init)\n",
    "    bounds = [(0, 0)] + [(None, None)] * (len(x_init)-2) + [(1, 1)]\n",
    "\n",
    "    def increasing_constraint(x, i):\n",
    "        return x[i+1] - x[i] \n",
    "\n",
    "    constraints = [{'type': 'ineq', 'fun': increasing_constraint, 'args': (i,)} for i in range(n-1)]\n",
    "\n",
    "    options = {\n",
    "        'disp': True,\n",
    "        'ftol': 1e-9,\n",
    "        'maxiter': 1000,  \n",
    "        'eps': 1.4901161193847656e-08 \n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    for i in range(niter):  # Run the optimization 10 times\n",
    "        x_init = np.random.rand(*x_init.shape)\n",
    "        result = minimize(cost, \n",
    "                        x_init, \n",
    "                        method = 'SLSQP', \n",
    "                        bounds = bounds, \n",
    "                        constraints=constraints,\n",
    "                        options=options,\n",
    "                        )\n",
    "        results.append(result)\n",
    "\n",
    "        print(f\"Iteration {i} done, cost: {result.fun}\")\n",
    "\n",
    "\n",
    "    # print(f\"Converged: {result.success}\")\n",
    "    # print(f\"Message: {result.message}\")\n",
    "\n",
    "    return min(results, key=lambda result: result.fun).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_optimal_nodes(x_init, cost):\n",
    "#     n = len(x_init)\n",
    "#     bounds = [(0, 1) for _ in range(n)]\n",
    "\n",
    "#     def increasing_constraint(x, i):\n",
    "#         return x[i+1] - x[i] \n",
    "\n",
    "#     constraints = [{'type': 'ineq', 'fun': increasing_constraint, 'args': (i,)} for i in range(n-1)]\n",
    "\n",
    "#     from scipy.optimize import differential_evolution\n",
    "\n",
    "#     result = differential_evolution(cost, bounds, maxiter = 10)\n",
    "\n",
    "#     # print(f\"Converged: {result.success}\")\n",
    "#     # print(f\"Message: {result.message}\")\n",
    "\n",
    "#     return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phi = lambda x: x\n",
    "# phi = lambda x: x**2\n",
    "phi = lambda x: np.sqrt(x)\n",
    "\n",
    "# Generate data to fit against\n",
    "N = 40\n",
    "n = 40\n",
    "x_data = np.linspace(0, 1, N)\n",
    "x_init = np.linspace(0, 1, n)\n",
    "\n",
    "f_1, f_2, f_3 = f1, f2, f5\n",
    "\n",
    "cost = C_factory(f_1, f_2, f_3, x_data, phi)\n",
    "\n",
    "print(cost(x_init))\n",
    "\n",
    "x_opt = find_optimal_nodes(x_init, cost)\n",
    "print(x_opt)\n",
    "\n",
    "print(cost(x_opt))\n",
    "\n",
    "plotting_results(f_1,f_2,f_3,phi,x_opt)\n",
    "\n",
    "plt.plot(x_init, phi(x_init), c = 'b')\n",
    "plt.scatter(x_init, x_opt, c = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want to do the same with curves in SO3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    y=1/(1+x**2)\n",
    "    return y\n",
    "\n",
    "def f2(x):\n",
    "    y=np.cos(2*np.pi*x)\n",
    "    return y\n",
    "\n",
    "def f3(x):\n",
    "    y=np.exp(3*x)*np.sin(2*x)\n",
    "    return y\n",
    "\n",
    "def f4(x):\n",
    "    y=np.sinc(x)\n",
    "    return y\n",
    "\n",
    "def f5(x):\n",
    "    y=3/4*(np.exp(-((9*x-2)**2)/4)+np.exp(((9*x+1)**2)/49))\n",
    "    y=y+1/2*np.exp(-((9*x-7)**2)/4)-1/10*np.exp(-(9*x-4)**2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import logm and expm\n",
    "import numpy as np\n",
    "from scipy.linalg import expm, logm\n",
    "\n",
    "def logm_(R):\n",
    "    \"\"\"\n",
    "    As logm don't allow for vectorization\n",
    "    Thus, use this for R.shape = (n, 3, 3)\n",
    "    \"\"\"\n",
    "    w_hat = np.zeros_like(R)\n",
    "    for i, R_i in enumerate(R):\n",
    "        w_hat[i] = logm(R_i)\n",
    "    return w_hat\n",
    "\n",
    "def hat(w): \n",
    "    if len(w.shape) == 2:  \n",
    "        return np.array([hat(w_i) for w_i in w])\n",
    "    if w.shape == (3,):\n",
    "        return np.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n",
    "    else:\n",
    "        raise ValueError(f'Invalid shape: {w.shape}')\n",
    "\n",
    "def vee(w_hat):\n",
    "    if w_hat.shape == (3,3): \n",
    "        return np.array([np.real(w_hat[2, 1]), np.real(w_hat[0, 2]), np.real(w_hat[1, 0])])\n",
    "    if w_hat.shape[-2:] == (3,3): \n",
    "        return np.array([vee(w_hat[i]) for i in range(w_hat.shape[0])])\n",
    "    raise ValueError(f\"w_hat must be of shape (3,3), not {w_hat.shape}\")\n",
    "\n",
    "\n",
    "def f_SO3(t):\n",
    "    f_1 = lambda t: np.cos(t)\n",
    "    f_2 = f2\n",
    "    # f_2 = lambda t: np.exp(t)\n",
    "    f_3 = lambda t: np.sin(t)\n",
    "    if isinstance(t, float):\n",
    "        x = f_1(t)\n",
    "        y = f_2(t)\n",
    "        z = f_3(t)\n",
    "        R = expm(hat(np.array([x, y, z])))\n",
    "\n",
    "        assert np.allclose(np.dot(R, R.T), np.eye(3)), 'R is not orthogonal'\n",
    "        assert np.isclose(np.linalg.det(R), 1), 'R is not special orthogonal'\n",
    "        return R\n",
    "\n",
    "    w = np.zeros((len(t), 3))\n",
    "    w[:,0] = f_1(t)\n",
    "    w[:,1] = f_2(t)\n",
    "    w[:,2] = f_3(t)\n",
    "    w_hat = hat(w)\n",
    "    R = expm(w_hat)\n",
    "\n",
    "    for i in range(R.shape[0]):\n",
    "        assert np.allclose(np.dot(R[i], R[i].T), np.eye(3)), 'R is not orthogonal'\n",
    "        assert np.isclose(np.linalg.det(R[i]), 1), 'R is not special orthogonal'\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1(t):\n",
    "    if isinstance(t, float):\n",
    "        print(logm(f_SO3(t)).shape)\n",
    "        return vee(logm(f_SO3(t)))[0]\n",
    "    return vee(logm_(f_SO3(t)))[:,0]\n",
    "\n",
    "def q2(t):\n",
    "    if isinstance(t, float):\n",
    "        return vee(logm(f_SO3(t)))[1]\n",
    "    return vee(logm_(f_SO3(t)))[:,1]\n",
    "\n",
    "def q3(t):\n",
    "    if isinstance(t, float):\n",
    "        return vee(logm(f_SO3(t)))[2]\n",
    "    return vee(logm_(f_SO3(t)))[:,2]\n",
    "\n",
    "t = np.array([0.2,0.3])\n",
    "# t = 0.2\n",
    "print(q1(t))\n",
    "print(q2(t))\n",
    "print(q3(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# t = np.linspace(0,1,10)\n",
    "# n_loops = 100\n",
    "# R = f_SO3(t)\n",
    "\n",
    "# w_hat = np.zeros((R.shape[0], 3, 3))\n",
    "\n",
    "# start = time.time()\n",
    "# for _ in range(n_loops):\n",
    "#     w_hat = logm_(R) \n",
    "#     # for i in range(R.shape[0]): \n",
    "#     #     w_hat[i] = logm(R[i])\n",
    "# print(\"Time for logm:\", time.time()-start)\n",
    "\n",
    "# start = time.time()\n",
    "# for i in range(n_loops): \n",
    "#     expm(w_hat)\n",
    "# print(\"Time for expm:\", time.time()-start)\n",
    "\n",
    "# # Is R == expm(logm(R))?\n",
    "# R_hat = expm(w_hat)\n",
    "# print(np.allclose(R, R_hat))\n",
    "# print(R_hat.shape)\n",
    "# print(R.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phi = lambda x: x\n",
    "phi = lambda x: x**2\n",
    "# phi = lambda x: np.sqrt(x)\n",
    "\n",
    "# Generate data to fit against\n",
    "N = 10\n",
    "x_data = np.linspace(0, 1, N)\n",
    "\n",
    "x_init = x_data.copy()\n",
    "cost = C_factory(q1, q2, q3, x_data, phi)\n",
    "\n",
    "x_opt = find_optimal_nodes(x_init, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_results(q1, q2, q3, phi, x_opt)\n",
    "\n",
    "# Plot the parameterization\n",
    "plt.plot(x_init, x_init)\n",
    "plt.plot(x_init, phi(x_init), c = 'b')\n",
    "plt.scatter(x_init, x_opt, c = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can assume that we start with two datasets \n",
    "# (x, y1) and (x,y2), or just assume that we have the two sequences y1 and y2\n",
    "\n",
    "# First create datapoints \n",
    "for n in [2, 4, 8, 16, 32]:\n",
    "    x = np.linspace(0,1,n)\n",
    "    y1 = f_SO3(x)\n",
    "    y2 = f_SO3(phi(x))\n",
    "\n",
    "    # Second create the interpolation function\n",
    "    f1_spline = CubicSpline(x, y1)\n",
    "    f2_spline = CubicSpline(x, y2)\n",
    "\n",
    "    # Check difference from analytical solution\n",
    "    x_fine = np.linspace(0,1,100)\n",
    "    print(n, np.linalg.norm(f1_spline(x_fine) - f_SO3(x_fine)))\n",
    "    print(n, np.linalg.norm(f2_spline(x_fine) - f_SO3(phi(x_fine))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "def derivative_constraint(x):\n",
    "    return np.diff(x)\n",
    "\n",
    "def C_one_dim_factory(f1_):\n",
    "    \"\"\" \n",
    "    Assumes f is linear\n",
    "    \"\"\"\n",
    "    def C(x):\n",
    "        x_eq = np.linspace(0, 1, len(x))\n",
    "        spline = CubicSpline(x_eq, f(x))\n",
    "        yfine = spline(xfine)\n",
    "        y_approx = f(phi(xfine))\n",
    "        cost = np.mean(np.square(y_approx - yfine))\n",
    "        # diff = np.diff(x)\n",
    "        # reg = np.sum(np.where(diff < 0, diff, 0))\n",
    "        # reg = np.sum(np.diff(x) ** 2)\n",
    "        # reg = np.sum(np.diff(y_approx) ** 2)\n",
    "        return cost\n",
    "    return C\n",
    "\n",
    "\n",
    "def C_factory(f1, y2_1, y2_2, y2_3, x_evaluate):\n",
    "    def C(x):\n",
    "        x_eq = np.linspace(0, 1, len(x))\n",
    "\n",
    "        w = vee(logm_(f1(x)))\n",
    "\n",
    "        # Use the derivative in R^3 to interpolate\n",
    "        y1_1 = CubicSpline(x_eq, w[:,0])(x_evaluate)\n",
    "        y1_2 = CubicSpline(x_eq, w[:,1])(x_evaluate)\n",
    "        y1_3 = CubicSpline(x_eq, w[:,2])(x_evaluate)\n",
    "\n",
    "        # Calcualte the difference\n",
    "        c1 = np.mean(np.square(y1_1 - y2_1))\n",
    "        c2 = np.mean(np.square(y1_2 - y2_2))\n",
    "        c3 = np.mean(np.square(y1_3 - y2_3))\n",
    "\n",
    "        return c1 + c2 + c3\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data\n",
    "x = np.linspace(0,1,50)\n",
    "phi = lambda x: x**2\n",
    "y1, y2 = f_SO3(x), f_SO3(phi(x))\n",
    "w2 = vee(logm_(y2))\n",
    "\n",
    "# From now we assume that they are from the same shape space, but nothing more\n",
    "f1_spline = CubicSpline(x, y1)\n",
    "\n",
    "x_eval = np.linspace(0,1,len(x))\n",
    "dy2_1 = CubicSpline(x, w2[:,0])(x_eval)\n",
    "dy2_2 = CubicSpline(x, w2[:,1])(x_eval)\n",
    "dy2_3 = CubicSpline(x, w2[:,2])(x_eval)\n",
    "\n",
    "\n",
    "cost = C_factory(f1_spline, dy2_1, dy2_2, dy2_3, x_eval)\n",
    "x_opt = find_optimal_nodes(x, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameterization\n",
    "plt.plot(x, x)\n",
    "plt.plot(x, phi(x), c = 'b')\n",
    "plt.scatter(x, x_opt, c = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation, Slerp\n",
    "\n",
    "def create_slerp_f(x, c):\n",
    "    \"\"\"\n",
    "    x: list of floats, where c is evaluated\n",
    "    c: list of 3x3 rotations SO(3) \n",
    "    t: list of floats we want to return rotations from\n",
    "    \"\"\"\n",
    "    def f_slerp(t): \n",
    "        rotations = Rotation.from_matrix(c)\n",
    "        slerp = Slerp(x, rotations)\n",
    "        rotation = slerp(t)\n",
    "        return rotation.as_matrix()\n",
    "    return f_slerp\n",
    "\n",
    "# Example of use: \n",
    "\n",
    "x = np.linspace(0, 1, 10)\n",
    "y1 = f_SO3(x)\n",
    "f = create_slerp_f(x, y1)\n",
    "x_eval = np.linspace(0,1,100)\n",
    "print(np.linalg.norm(f_SO3(x_eval) - f(x_eval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pointswise derivative\n",
    "\n",
    "- X = vee(logm(R)) \n",
    "- X = [dx, dy, dz]\n",
    "- Take two curves, c_1, c_2 -> X_1, X_2\n",
    "- Interpolate CubicSpline(x, X_1), CubicSpline(x, X_2)\n",
    "- C(x_opt) = \\| CS(x, CS(x, X_1)(x_opt))(x_eval) - CS(x, X_2)(x_eval) \\|_L2\n",
    "- We then try to minimize the cost function, where x_opt is the optimal reparameterization of x. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autograd.numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.optimize import minimize\n",
    "# from scipy.optimize import NonlinearConstraint\n",
    "# from scipy.optimize import Bounds\n",
    "# from scipy.interpolate import CubicSpline\n",
    "\n",
    "# def C_factory(f1_, y2, x_evaluate):\n",
    "#     def C(x):\n",
    "#         x_eq = np.linspace(0, 1, len(x))\n",
    "#         x = np.where(x > 1, 1, x)\n",
    "#         x = np.where(x < 0, 0, x)\n",
    "#         y1 = CubicSpline(x_eq, f1_(x))(x_evaluate)\n",
    "#         return np.sum(np.mean(np.square(y1 - y2), axis=0))\n",
    "#     return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the data\n",
    "# x = np.linspace(0,1,50)\n",
    "# phi = lambda x: x**2\n",
    "# y1, y2 = f_SO3(x), f_SO3(phi(x))\n",
    "\n",
    "# w1 = vee([logm(R) for R in y1])\n",
    "# w2 = vee([logm(R) for R in y2])\n",
    "\n",
    "# f1_spline = CubicSpline(x, w1)\n",
    "# f2_spline = CubicSpline(x, w2)\n",
    "\n",
    "# x_eval = np.linspace(0,1,len(x))\n",
    "\n",
    "# y2 = f2_spline(x_eval)\n",
    "# cost = C_factory(f1_spline, y2, x_eval)\n",
    "# x_opt = find_optimal_nodes(x, cost, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the parameterization\n",
    "# print(x_opt)\n",
    "# print(cost(x_opt))\n",
    "# print(cost(phi(x)))\n",
    "# plt.plot(x, x)\n",
    "# plt.plot(x, phi(x), c = 'b')\n",
    "# plt.scatter(x, x_opt, c = 'r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to change the derivative\n",
    "\n",
    "- use g_{i-1}^T g_i, i = 1, ..., n to find derivative\n",
    "- we do this on both, and then we interpolate the first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "\n",
    "# def right_log_derivative(I: np.ndarray, rotation: np.ndarray, index: float) -> np.ndarray:\n",
    "#     return scipy.linalg.logm(np.dot(rotation[index + 1], rotation[index].T)) / (I[index + 1] - I[index])\n",
    "\n",
    "# def right_log_single_rotation(I: np.ndarray, rotation: np.ndarray) -> np.ndarray:\n",
    "#     return np.array([right_log_derivative(I, rotation, j) \n",
    "#                      for j in range(I.shape[0] - 1)]).reshape(rotation.shape[0] - 1, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autograd.numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.optimize import minimize\n",
    "# from scipy.optimize import NonlinearConstraint\n",
    "# from scipy.optimize import Bounds\n",
    "# from scipy.interpolate import CubicSpline\n",
    "\n",
    "# from scipy.interpolate import interp1d\n",
    "\n",
    "# def interp3d(x,y): \n",
    "#     \"\"\"\n",
    "#     x.shape = (n,)\n",
    "#     y.shape = (n,3)\n",
    "#     \"\"\"\n",
    "#     def interp(x_eval):\n",
    "#         y1 = interp1d(x, y[:,0], kind='linear')(x_eval)\n",
    "#         y2 = interp1d(x, y[:,1], kind='linear')(x_eval)\n",
    "#         y3 = interp1d(x, y[:,2], kind='linear')(x_eval)\n",
    "#         return np.column_stack([y1, y2, y3])\n",
    "#     return interp\n",
    "\n",
    "# def derivative_constraint(x):\n",
    "#     return np.diff(x)\n",
    "\n",
    "# def C_factory(f1, y2, x_eval):\n",
    "#     def C(x):\n",
    "#         x = np.where(x > 1, 1, x)\n",
    "#         x = np.where(x < 0, 0, x)\n",
    "#         x_eq = np.linspace(0, 1, len(x))\n",
    "#         y1 = interp3d(x_eq, f1(x))(x_eval)\n",
    "#         return np.sum(np.mean(np.square(y1 - y2), axis=0))\n",
    "#     return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Create the data\n",
    "# x = np.linspace(0,1,100)\n",
    "# phi = lambda x: x**2\n",
    "# g1, g2 = f_SO3(x), f_SO3(phi(x))\n",
    "\n",
    "# xi_1 = vee(right_log_single_rotation(x, g1))\n",
    "# xi_2 = vee(right_log_single_rotation(x, g2))\n",
    "\n",
    "\n",
    "# # Add [0,0,0] into xi_1 and xi_2\n",
    "# xi_1 = np.concatenate([np.zeros((1, 3)), xi_1], axis=0)\n",
    "# xi_2 = np.concatenate([np.zeros((1, 3)), xi_2], axis=0)\n",
    "\n",
    "# f1_int = interp3d(x, xi_1)\n",
    "# f2_int = interp3d(x, xi_2)\n",
    "\n",
    "\n",
    "# x_eval = np.linspace(0,1,len(x))\n",
    "# y2 = f2_int(x_eval)\n",
    "# cost = C_factory(f1_int, y2, x_eval)\n",
    "# x_opt = find_optimal_nodes(x, cost, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the parameterization\n",
    "# print(cost(x_opt))\n",
    "# print(cost(phi(x)))\n",
    "# plt.plot(x, x)\n",
    "# plt.plot(x, phi(x), c = 'b')\n",
    "# plt.scatter(x, x_opt, c = 'r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe just use SLERP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_difference(y1, y2):\n",
    "    assert y1.shape == y2.shape, 'y1 and y2 must have the same shape'\n",
    "    diffs = np.array([logm(y1[i].T @ y2[i]) for i in range(y1.shape[0])])\n",
    "    norm_sq = np.mean(np.linalg.norm(diff, 'fro')**2 for diff in diffs)\n",
    "    return norm_sq\n",
    "\n",
    "\n",
    "def C_factory(f1, y2, x_evaluate):\n",
    "    def C(x):\n",
    "        x_eq = np.linspace(0, 1, len(x))\n",
    "        x = np.where(x > 1, 1, x)\n",
    "        x = np.where(x < 0, 0, x)\n",
    "\n",
    "        y1 = create_slerp_f(x_eq, f1(x))(x_evaluate)\n",
    "        return rotation_difference(y1, y2)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data\n",
    "x = np.linspace(0,1,10)\n",
    "phi = lambda x: x**2\n",
    "y1, y2 = f_SO3(x), f_SO3(phi(x))\n",
    "\n",
    "f1_slerp = create_slerp_f(x, y1)\n",
    "f2_slerp = create_slerp_f(x, y2)\n",
    "\n",
    "x_eval = np.linspace(0,1,len(x))\n",
    "\n",
    "y2 = f2_slerp(x_eval)\n",
    "\n",
    "cost = C_factory(f1_slerp, y2, x_eval)\n",
    "x_opt = find_optimal_nodes(x, cost, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameterization\n",
    "print(cost(x_opt))\n",
    "print(cost(phi(x)))\n",
    "plt.plot(x, x)\n",
    "plt.plot(x, phi(x), c = 'b')\n",
    "plt.scatter(x, x_opt, c = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to do it without SLERP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from threading import Thread\n",
    "# from scipy.optimize import minimize\n",
    "# import numpy as np\n",
    "\n",
    "# def optimize(args):\n",
    "#     i, x_init, cost, bounds, constraints, options = args\n",
    "#     x_init = np.random.rand(*x_init.shape)  # Reinitialize x_init for each thread\n",
    "#     result = minimize(cost, x_init, method='SLSQP', bounds=bounds, constraints=constraints, options=options)\n",
    "#     print(f\"Iteration {i} done, cost: {result.fun}\")\n",
    "#     return result\n",
    "\n",
    "# def find_optimal_nodes(x_init, cost, niter=10):\n",
    "#     n = len(x_init)\n",
    "#     bounds = [(0, 0)] + [(None, None)] * (n-2) + [(1, 1)]\n",
    "#     def increasing_constraint(x, i):\n",
    "#         return x[i+1] - x[i] \n",
    "#     constraints = [{'type': 'ineq', 'fun': increasing_constraint, 'args': (i,)} for i in range(n-1)]\n",
    "    \n",
    "#     options = {\n",
    "#         'disp': True,\n",
    "#         'ftol': 1e-9,\n",
    "#         'maxiter': 1000,\n",
    "#         'eps': 1.4901161193847656e-08\n",
    "#     }\n",
    "    \n",
    "#     threads = []\n",
    "#     results = []\n",
    "\n",
    "#     for args in [(i, x_init, cost, bounds, constraints, options) for i in range(niter)]:\n",
    "#         thread = Thread(target=lambda q, arg: q.append(optimize(arg)), args=(results, args))\n",
    "#         threads.append(thread)\n",
    "#         thread.start()\n",
    "\n",
    "#     for thread in threads:\n",
    "#         thread.join()\n",
    "\n",
    "#     return min(results, key=lambda result: result.fun).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import expm, logm\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "x = np.linspace(0,1,20)\n",
    "g = f_SO3(x)\n",
    "w_hat = [logm(R) for R in g]\n",
    "f_s = CubicSpline(x, w_hat)\n",
    "g_approx = expm(f_s(x))\n",
    "print(np.linalg.norm(g - g_approx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation_SO3(x, g):\n",
    "    # Calculate xi_hat outside the function for efficiency\n",
    "    xi_hat = np.array([logm(g[i-1].T @ g[i]) for i in range(1, len(g))])\n",
    "\n",
    "    def interpSO3(x_eval):\n",
    "        # Find where x_eval comes in x\n",
    "        indices = np.searchsorted(x, x_eval, side='right')\n",
    "        \n",
    "        # Handle out of bounds by clamping to valid range\n",
    "        indices = np.clip(indices, 1, len(x) - 1)\n",
    "        # Interpolation calculation\n",
    "        alpha = (x_eval - x[indices-1]) / (x[indices] - x[indices-1])\n",
    "\n",
    "        result = g[indices-1] @ expm(alpha[:, np.newaxis, np.newaxis] * xi_hat[indices-1])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return interpSO3\n",
    "\n",
    "def calculate_varphi_dot(x, T): \n",
    "    x_eq = np.linspace(0, T, len(x))\n",
    "    phi_dot = np.gradient(x) / np.gradient(x_eq)\n",
    "    return phi_dot\n",
    "\n",
    "def linear_interpolation_SO3_reparam(x, g):\n",
    "    # Calculate xi_hat outside the function for efficiency\n",
    "    xi_hat = np.array([logm(g[i-1].T @ g[i]) for i in range(1, len(g))])\n",
    "    \n",
    "    # Precompute varphi_dot if possible\n",
    "    varphi_dot = calculate_varphi_dot(x, T = 1)\n",
    "\n",
    "    def interpSO3(x_eval):\n",
    "        # Find where x_eval comes in x\n",
    "        indices = np.searchsorted(x, x_eval, side='right')\n",
    "        \n",
    "        # Handle out of bounds by clamping to valid range\n",
    "        indices = np.clip(indices, 1, len(x) - 1)\n",
    "        # Interpolation calculation\n",
    "        alpha = (x_eval - x[indices-1]) / (x[indices] - x[indices-1])\n",
    "        \n",
    "        # Adjust alpha with varphi_dot if needed\n",
    "        adjusted_alpha = alpha * varphi_dot[indices-1]\n",
    "        \n",
    "        # Expand dimensions for matrix operations\n",
    "        adjusted_alpha_expanded = adjusted_alpha[:, np.newaxis, np.newaxis]\n",
    "        \n",
    "        # Perform the interpolation\n",
    "        result = g[indices-1] @ expm(adjusted_alpha_expanded * xi_hat[indices-1])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return interpSO3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = linear_interpolation_SO3(x, g)\n",
    "x_eval = np.linspace(0,1,5)\n",
    "\n",
    "g_approx = f(x_eval)\n",
    "print(np.linalg.norm(f_SO3(x_eval) - g_approx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_difference(y1, y2):\n",
    "    assert y1.shape == y2.shape, 'y1 and y2 must have the same shape'\n",
    "    diffs = np.array([logm(y1[i].T @ y2[i]) for i in range(y1.shape[0])])\n",
    "    norm_sq = np.mean(np.array([np.linalg.norm(diff, 'fro')**2 for diff in diffs]))\n",
    "    return norm_sq\n",
    "\n",
    "\n",
    "def C_factory(f1, y2, x_evaluate):\n",
    "    def C(x):\n",
    "        x_eq = np.linspace(0, 1, len(x))\n",
    "        x = np.where(x > 1, 1, x)\n",
    "        x = np.where(x < 0, 0, x)\n",
    "\n",
    "        y1 = linear_interpolation_SO3_reparam(x_eq, f1(x))(x_evaluate)\n",
    "        return rotation_difference(y1, y2)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Create the data\n",
    "x = np.linspace(0,1,20)\n",
    "phi = lambda x: x**2\n",
    "y1, y2 = f_SO3(x), f_SO3(phi(x))\n",
    "\n",
    "f1_interp = linear_interpolation_SO3(x, y1)\n",
    "f2_interp = linear_interpolation_SO3(x, y2)\n",
    "\n",
    "x_eval = np.linspace(0,1,len(x))\n",
    "\n",
    "y2 = f2_interp(x_eval)\n",
    "\n",
    "cost = C_factory(f1_interp, y2, x_eval)\n",
    "\n",
    "print(\"Start Opt\")\n",
    "x_opt = find_optimal_nodes(x, cost, 3)\n",
    "print(\"End Opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the parameterization\n",
    "print(cost(x_opt))\n",
    "print(cost(phi(x)))\n",
    "plt.plot(x, x)\n",
    "plt.plot(x, phi(x), c = 'b')\n",
    "plt.scatter(x, x_opt, c = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_or = np.linspace(0,1,10)\n",
    "x_phi = np.linspace(0,1,10) ** 2\n",
    "x_phi_dot = 2 * np.linspace(0,1,10)\n",
    "x_phi_dot_forward = np.diff(x_phi) / np.diff(x_or)\n",
    "x_phi_dot_central = np.gradient(x_phi) / np.gradient(x_or)\n",
    "\n",
    "plt.plot(x_or, x_phi_dot_central, label='Central', color = 'red')\n",
    "plt.plot(x_or, x_phi_dot, label='Analytical', linestyle='-.')\n",
    "plt.scatter(np.linspace(0,1, len(x_opt)), calculate_varphi_dot(x_opt, T = 1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Create the data\n",
    "x = np.linspace(0,1,20)\n",
    "phi = lambda x: np.sqrt(x)\n",
    "y1, y2 = f_SO3(x), f_SO3(phi(x))\n",
    "\n",
    "f1_interp = linear_interpolation_SO3(x, y1)\n",
    "f2_interp = linear_interpolation_SO3(x, y2)\n",
    "\n",
    "x_eval = np.linspace(0,1,len(x))\n",
    "\n",
    "y2 = f2_interp(x_eval)\n",
    "\n",
    "cost = C_factory(f1_interp, y2, x_eval)\n",
    "\n",
    "print(\"Start Opt\")\n",
    "x_opt = find_optimal_nodes(x, cost, 5)\n",
    "print(\"End Opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the parameterization\n",
    "print(cost(x_opt))\n",
    "print(cost(phi(x)))\n",
    "plt.plot(x, x)\n",
    "plt.plot(x, phi(x), c = 'b')\n",
    "plt.scatter(x, x_opt, c = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Create the data\n",
    "x = np.linspace(0,1,20)\n",
    "phi = lambda x: np.sin(x) / np.sin(1)\n",
    "y1, y2 = f_SO3(x), f_SO3(phi(x))\n",
    "\n",
    "f1_interp = linear_interpolation_SO3(x, y1)\n",
    "f2_interp = linear_interpolation_SO3(x, y2)\n",
    "\n",
    "x_eval = np.linspace(0,1,len(x))\n",
    "\n",
    "y2 = f2_interp(x_eval)\n",
    "\n",
    "cost = C_factory(f1_interp, y2, x_eval)\n",
    "\n",
    "print(\"Start Opt\")\n",
    "x_opt = find_optimal_nodes(x, cost, 10)\n",
    "print(\"End Opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the parameterization\n",
    "print(cost(x_opt))\n",
    "print(cost(phi(x)))\n",
    "plt.plot(x, x)\n",
    "plt.plot(x, phi(x), c = 'b')\n",
    "plt.scatter(x, x_opt, c = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit as function of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineLayer(nn.Module):\n",
    "    def __init__(self, m, input_dim):\n",
    "        super(SineLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(m, input_dim) * 0.01)\n",
    "        self.m = m\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        k_lst = torch.arange(1, self.m+1).unsqueeze(-1)  # (m, 1)\n",
    "        x_expanded = x.unsqueeze(1)  # (n, 1, input_dim) where n is the batch size\n",
    "        sine_basis = torch.sin(k_lst * torch.pi * x_expanded) / (k_lst * torch.pi)  # (m, n, input_dim)\n",
    "        weighted_sum = sine_basis * self.weights.unsqueeze(0)  # Broadcasting weights: (1, m, input_dim)\n",
    "        return x + torch.sum(weighted_sum, dim=1)  # Sum over m, output shape: (n, input_dim)\n",
    "\n",
    "\n",
    "class NeuralNetworkApproximation(nn.Module):\n",
    "    def __init__(self, input_dim, L, m):\n",
    "        super(NeuralNetworkApproximation, self).__init__()\n",
    "        self.layers = nn.ModuleList([SineLayer(m, input_dim) for _ in range(L)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def train_model(self, inputs, targets, epochs=100, lr=0.01):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.linspace(0, 1, 20).unsqueeze(0).repeat(10, 1)\n",
    "targets = inputs**2\n",
    "\n",
    "model = NeuralNetworkApproximation(input_dim=20, L=10, m = 40)\n",
    "model.train_model(inputs, targets, epochs=500, lr=0.001)\n",
    "\n",
    "test_x = torch.linspace(0, 1, 20).unsqueeze(0)  # Add an extra dimension\n",
    "print(test_x.shape)\n",
    "test_y = model(test_x)\n",
    "print(test_y.shape)\n",
    "#print(test_y.detach().numpy())\n",
    "\n",
    "plt.plot(test_x.numpy()[0], test_y.detach().numpy()[0], label='Approximation')  # Plot the first output\n",
    "plt.plot(test_x.numpy()[0], test_x.numpy()[0]**2, label=\"True\", linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want to be able to reparameterize the curve\n",
    "\n",
    "- It should return the optimal t_opt\n",
    "- Will use a function. f.ex. sin(t) \n",
    "- Then create several phi(t) \n",
    "- c = sin(phi(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineLayer(nn.Module):\n",
    "    def __init__(self, m, input_dim):\n",
    "        super(SineLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(m, input_dim) * 0.01)  # (m, input_dim)\n",
    "        self.m = m\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() != 1:\n",
    "            raise ValueError(f\"Input must be a 1D vector, got shape {x}\")\n",
    "\n",
    "        k_lst = torch.arange(1, self.m + 1, device=x.device)\n",
    "        x_expanded = x.unsqueeze(1)  # Expand x to (input_dim, 1)\n",
    "        sine_basis = torch.sin(k_lst * torch.pi * x_expanded) / (k_lst * torch.pi)  # (input_dim, m)\n",
    "        weighted_sum = sine_basis * self.weights.t()  # Transpose weights to (input_dim, m)\n",
    "        return x + torch.sum(weighted_sum, axis=1)  # Sum across sine components, reduce back to (input_dim,)\n",
    "\n",
    "def linear_interpolation(x, y, x_opt):\n",
    "    idx = torch.searchsorted(x, x_opt) - 1\n",
    "    idx = idx.clamp(0, len(x) - 2) \n",
    "\n",
    "    x0, x1 = x[idx], x[idx + 1]\n",
    "    y0, y1 = y[idx], y[idx + 1]\n",
    "\n",
    "    y_opt = y0 + (x_opt - x0) * (y1 - y0) / (x1 - x0)\n",
    "    return y_opt\n",
    "\n",
    "class NeuralNetworkApproximation(nn.Module):\n",
    "    def __init__(self, input_dim, L, m):\n",
    "        super(NeuralNetworkApproximation, self).__init__()\n",
    "        self.layers = nn.ModuleList([SineLayer(m, input_dim) for _ in range(L)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def train_model(self, inputs, targets, epochs=100, lr=0.01):\n",
    "        criterion = nn.MSELoss()  \n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr) \n",
    "        x_init = torch.linspace(0, 1, len(inputs))\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad() \n",
    "            x_opt = self(inputs) \n",
    "            y_opt = linear_interpolation(x_init, inputs, x_opt)\n",
    "            loss = criterion(y_opt, targets)  \n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(0, 1, 20)\n",
    "phi = lambda x: torch.sqrt(x)\n",
    "f = lambda x: torch.sin(x) \n",
    "inputs = f(t) \n",
    "targets = f(phi(t))\n",
    "\n",
    "print(inputs.shape)\n",
    "print(targets.shape)\n",
    "model = NeuralNetworkApproximation(input_dim=20, L=10, m = 40)\n",
    "model.train_model(inputs, targets, epochs=1000, lr=0.001)\n",
    "\n",
    "test_x = torch.linspace(0, 1, 20)\n",
    "x_phi = model(f(test_x))\n",
    "new_y = linear_interpolation(t, inputs, x_phi).detach().numpy()\n",
    "\n",
    "plt.plot(t, new_y, label = 'approx')\n",
    "plt.plot(t, f(phi(t)), label = 'true')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to be able to perform it on several vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation_2d(x, y, x_opt):\n",
    "    y_opt = torch.zeros_like(x_opt)\n",
    "    \n",
    "    # Step 1: Get indices for interpolation\n",
    "    idx = torch.searchsorted(x, x_opt) - 1\n",
    "    idx = idx.clamp(0, x.size(1) - 2)  # Ensure indices are within the valid range\n",
    "    \n",
    "    x0 = torch.gather(x, 1, idx)\n",
    "    x1 = torch.gather(x, 1, idx + 1)\n",
    "    y0 = torch.gather(y, 1, idx)\n",
    "    y1 = torch.gather(y, 1, idx + 1)\n",
    "    \n",
    "    # Step 3: Compute the interpolated values\n",
    "    y_opt = y0 + (x_opt - x0) * (y1 - y0) / (x1 - x0)\n",
    "    \n",
    "    return y_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_params(c, y_min, epsilon):\n",
    "    scaling_factor = 1 / (1 - min(0, y_min - epsilon))\n",
    "    return c * scaling_factor\n",
    "\n",
    "\n",
    "class SineLayer(nn.Module):\n",
    "    def __init__(self, m, input_dim):\n",
    "        super(SineLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(m, input_dim) * 0.01)\n",
    "        self.m = m\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        if z.dim() != 2:\n",
    "            raise ValueError(f\"Input must be a 2D batched tensor, got shape {x.shape}\")\n",
    "\n",
    "        # Project parameters if necessary\n",
    "        y_min = y.min().item()\n",
    "        for param in self.parameters():\n",
    "            param.data = project_params(param.data, y_min, 1e-6)\n",
    "\n",
    "        z_expanded = z.unsqueeze(1)\n",
    "        k_lst = torch.arange(1, self.m + 1, device=z.device).view(1, -1, 1)\n",
    "        sine_basis = torch.sin(k_lst * torch.pi * z_expanded) / (k_lst * torch.pi)\n",
    "        cosine_basis = torch.cos(k_lst * torch.pi * z_expanded)\n",
    "        weighted_sine = torch.einsum('bmi,mi->bi', sine_basis, self.weights)\n",
    "        weighted_cosine = torch.einsum('bmi,mi->bi', cosine_basis, self.weights)\n",
    "        return z + weighted_sine, (torch.ones_like(weighted_cosine) + weighted_cosine) * y\n",
    "\n",
    "class NeuralNetworkApproximation(nn.Module):\n",
    "    def __init__(self, input_dim, L, m):\n",
    "        super(NeuralNetworkApproximation, self).__init__()\n",
    "        self.layers = nn.ModuleList([SineLayer(m, input_dim) for _ in range(L)])\n",
    "\n",
    "    def forward(self, z):\n",
    "        y = torch.ones_like(z) \n",
    "        for layer in self.layers:\n",
    "            z, y = layer(z, y)\n",
    "        return z, y\n",
    "\n",
    "\n",
    "    def train_model(self, inputs, targets, epochs=100, lr=0.01):\n",
    "        criterion = nn.MSELoss()  \n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr) \n",
    "        x_init = torch.linspace(0, 1, inputs.shape[1]).repeat(inputs.shape[0], 1)\n",
    "\n",
    "        print(\"Input\", inputs.shape)\n",
    "        print(\"x_init\", x_init.shape)\n",
    "            \n",
    "        for epoch in range(epochs):            \n",
    "            z, y = self(inputs)\n",
    "            sol = linear_interpolation_2d(x_init, inputs, z) * torch.sqrt(y)\n",
    "            loss = criterion(sol, targets)\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_perturbations(num_ts, t):\n",
    "    std_dev = (t[1] - t[0]) / 2\n",
    "\n",
    "    def cut(tensor, min_value, max_value):\n",
    "        return torch.clamp(tensor, min_value, max_value)\n",
    "\n",
    "    ts = []\n",
    "    for i in range(num_ts):\n",
    "        torch.manual_seed(i)\n",
    "        normal = torch.normal(0, std_dev, size=(len(t)-2,))\n",
    "        normal = cut(normal, -std_dev, std_dev)\n",
    "        t_clone = t.clone()\n",
    "        t_clone[1:-1] += normal\n",
    "        ts.append(t_clone)\n",
    "\n",
    "    return ts\n",
    "\n",
    "t = torch.linspace(0,1,100)\n",
    "t_s = create_perturbations(10, t)\n",
    "t1, t2, t3, t4, t5, t6, t7, t8, t9, t10 = t_s\n",
    "\n",
    "for i in range(10):\n",
    "    plt.plot(t,t_s[i])\n",
    "plt.plot(t,t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = 100\n",
    "t = torch.linspace(0, 1, num_datapoints)\n",
    "phi = lambda x: torch.sqrt(x)\n",
    "f = lambda x: torch.sin(3 * x) * torch.cos(x * 10)\n",
    "\n",
    "# inputs = torch.stack([f(t) for t in t_s])\n",
    "inputs = torch.stack([f(t)])\n",
    "targets = f(t4).repeat(len(inputs), 1)\n",
    "\n",
    "print(inputs.shape)\n",
    "print(targets.shape)\n",
    "\n",
    "\n",
    "model = NeuralNetworkApproximation(input_dim=num_datapoints, L=1, m=20)\n",
    "model.train_model(inputs, targets, epochs=1000, lr=0.01)\n",
    "\n",
    "test_x = torch.linspace(0, 1, num_datapoints)\n",
    "test_y = f(test_x).unsqueeze(0)\n",
    "x_phi, _ = model(test_y)\n",
    "x_phi = x_phi.squeeze(0)\n",
    "print(x_phi)\n",
    "\n",
    "new_y = linear_interpolation(test_x, test_y.squeeze(0), x_phi).detach().numpy()\n",
    "\n",
    "plt.plot(test_x, test_y.squeeze(0), label = 'start')\n",
    "plt.plot(t, new_y, label = 'approx')\n",
    "plt.scatter(t, f(t4), label = 'true')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = 100\n",
    "t = torch.linspace(0, 1, num_datapoints)\n",
    "phi = lambda x: torch.sqrt(x)\n",
    "f = lambda x: torch.sin(3 * x) * torch.cos(x * 10)\n",
    "\n",
    "inputs = torch.stack([f(t)])\n",
    "targets = f(t4).repeat(1, 1)\n",
    "\n",
    "print(inputs.shape)\n",
    "print(targets.shape)\n",
    "\n",
    "\n",
    "model = NeuralNetworkApproximation(input_dim=num_datapoints, L=10, m = 100)\n",
    "model.train_model(inputs, targets, epochs=3000, lr=0.001)\n",
    "\n",
    "test_x = torch.linspace(0, 1, num_datapoints)\n",
    "test_y = f(test_x).unsqueeze(0)\n",
    "x_phi = model(test_y).squeeze(0)\n",
    "\n",
    "print(x_phi)\n",
    "\n",
    "new_y = linear_interpolation(test_x, test_y.squeeze(0), x_phi).detach().numpy()\n",
    "\n",
    "plt.plot(t, new_y, label = 'approx')\n",
    "plt.scatter(t, f(t), label = 'true')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(0, 1, 1000)\n",
    "plt.plot(t, f(t))\n",
    "plt.scatter(t4, f(t4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(0, 1, 20)\n",
    "phi = lambda x: x**2\n",
    "f = lambda x: torch.sin(x)\n",
    "inputs = f(t)\n",
    "targets = f(phi(t))\n",
    "\n",
    "a = inputs\n",
    "b = targets\n",
    "\n",
    "print(linear_interpolation(a.unsqueeze(0),b.unsqueeze(0),a.unsqueeze(0)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineLayer(nn.Module):\n",
    "    def __init__(self, m, input_dim):\n",
    "        super(SineLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(m, input_dim) * 0.01)  # (m, input_dim)\n",
    "        self.m = m\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() != 1:\n",
    "            raise ValueError(f\"Input must be a 1D vector, got shape {x}\")\n",
    "\n",
    "        k_lst = torch.arange(1, self.m + 1, device=x.device)\n",
    "        x_expanded = x.unsqueeze(1)  # Expand x to (input_dim, 1)\n",
    "        sine_basis = torch.sin(k_lst * torch.pi * x_expanded) / (k_lst * torch.pi)  # (input_dim, m)\n",
    "        weighted_sum = sine_basis * self.weights.t()  # Transpose weights to (input_dim, m)\n",
    "        return x + torch.sum(weighted_sum, axis=1)  # Sum across sine components, reduce back to (input_dim,)\n",
    "\n",
    "# Testing the layer:\n",
    "input_dim = 20\n",
    "m = 40\n",
    "layer = SineLayer(m, input_dim)\n",
    "x = torch.randn(input_dim)  # Single sample with input_dim features\n",
    "output = layer(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineLayer(nn.Module):\n",
    "    def __init__(self, m, input_dim):\n",
    "        super(SineLayer, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(m, input_dim) * 0.01)  # (m, input_dim)\n",
    "        self.m = m\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() != 1:\n",
    "            raise ValueError(f\"Input must be a 1D vector, got shape {x}\")\n",
    "\n",
    "        k_lst = torch.arange(1, self.m + 1, device=x.device)\n",
    "        x_expanded = x.unsqueeze(1)  # Expand x to (input_dim, 1)\n",
    "        sine_basis = torch.sin(k_lst * torch.pi * x_expanded) / (k_lst * torch.pi)  # (input_dim, m)\n",
    "        weighted_sum = sine_basis * self.weights.t()  # Transpose weights to (input_dim, m)\n",
    "        return x + torch.sum(weighted_sum, axis=1)  # Sum across sine components, reduce back to (input_dim,)\n",
    "\n",
    "# Testing the layer:\n",
    "input_dim = 20\n",
    "m = 40\n",
    "layer = SineLayer(m, input_dim)\n",
    "x = torch.randn(input_dim)  # Single sample with input_dim features\n",
    "output = layer(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = lambda x: x\n",
    "f2 = lambda x: x**2\n",
    "f3 = lambda x: x**3\n",
    "f4 = lambda x: x**(1/2)\n",
    "f5 = lambda x: x**(1/3)\n",
    "f6 = lambda x: np.sin(x) / np.sin(1)\n",
    "f7 = lambda x: (np.exp(x) - 1) / (np.exp(1) - 1)\n",
    "\n",
    "f_lst = [f1, f2, f3, f4, f5, f6, f7]\n",
    "x = np.linspace(0,1,100)\n",
    "for f in f_lst:\n",
    "    plt.plot(x, f(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.linspace(0, 1, 20).unsqueeze(0).repeat(10, 1)\n",
    "targets = inputs**2\n",
    "\n",
    "model = NeuralNetworkApproximation(input_dim=20, L=10, m = 40)\n",
    "model.train_model(inputs, targets, epochs=500, lr=0.001)\n",
    "\n",
    "test_x = torch.linspace(0, 1, 20).unsqueeze(0)  # Add an extra dimension\n",
    "print(test_x.shape)\n",
    "test_y = model(test_x)\n",
    "print(test_y.shape)\n",
    "#print(test_y.detach().numpy())\n",
    "\n",
    "plt.plot(test_x.numpy()[0], test_y.detach().numpy()[0], label='Approximation')  # Plot the first output\n",
    "plt.plot(test_x.numpy()[0], test_x.numpy()[0]**2, label=\"True\", linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
